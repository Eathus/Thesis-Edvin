
  {
    "role": "user",
    "content": "This is a GitHub Issue\nrepo: ZipArchive\nowner: ZipArchive\nTitle : Unhandled edge case in _sanitizedPath #680 \n\n--- start body ---\nDescription:\n\nThere is an unhandeled edge case where we can crash the application when it tries to unzip a specially crafted zip file\n\nTechnical details:\n\nBelow is a code snippet from the _sanitizedPath function used to sanitize file paths coming from zip entries before extraction to prevent path traversal, the function prepends file:/// to the provided path, standardizes it using NSURL and then removes the prepended file:///, when provided with the following path however /.., the standardized path returned by NSURL becomes file://, which has 7 characters, the package however expects at least 8 characters (length of the prepended text), this results in crashing the application.\n\n// Add scheme \"file:///\" to support sanitation on names with a colon like \"file:a/../../../usr/bin\"\nstrPath = [@\"file:///\" stringByAppendingString:strPath];\n\n// Sanitize path traversal characters to prevent directory backtracking. Ignoring these characters mimicks the default behavior of the Unarchiving tool on macOS.\n// \"../../../../../../../../../../../tmp/test.txt\" -> \"tmp/test.txt\"\n// \"a/b/../c.txt\" -> \"a/c.txt\"\nstrPath = [NSURL URLWithString:strPath].standardizedURL.absoluteString;\n\n// Remove the \"file:///\" scheme\nstrPath = [strPath substringFromIndex:8];\nPoC:\n\nimport zipfile\n\ndef compress_file(filename):\n    with zipfile.ZipFile('payload.zip', 'w') as zipf:\n        zipf.writestr(filename, \"Test payload\")\n\nfilename = '/..'\n\ncompress_file(filename)\n--- end body ---\n\ncomments :\n--- start comments ---\n\n--- end comments ---"
  },
  {
    "role": "assistant",
    "content": " { \"gpt_description\": \"An unhandled edge case in the component _sanitizedPath of ZipArchive v2.5.4 allows attackers to cause a Denial of Service (DoS) via a crafted zip file.\" \"gpt_confidence\": 5, \"gpt_is_relevant\": true }"
  },
  {
    "role": "user",
    "content": "This is a GitHub Issue\nrepo:MonetDB\nowner:MonetDB\nTitle : MonetDB server 11.46.0 crashes at list_append #7383\n\nIssue date:\n\n--- start body ---\nDescribe the bug\nMonetDB server 11.46.0 crashes at list_append after executing SQL statements through mclient.\n\nTo Reproduce\n\ncreate table t1(c1 int auto_increment primary key NOT NULL);\ncreate trigger i1 after insert on t1 for each row insert into t1 values(NULL);\ninsert into t1 values(NULL);\nExpected behavior\nThis crash is strange. From the backtrace as follows, we can know that the crash is caused by infinite recursion.\n\nHowever, if we change the first statement into create table t1(c1 int);, the MonetDB will return the message Query too complex: running out of stack space when executing the third stmt. So, MonetDB handles out of stack space, but it does not work with some special table definitions?\n\nWhat's more, the definition of the table contains the NOT NULL constraint, which should fail the third stmt. If we remove the create trigger statement, MonetDB will return the message INSERT INTO: NOT NULL constraint violated for column t1.c1 when executing the insert stmt. But with the create trigger stmt, it crashes instead.\n\nAnyway, I think the expected behavior is to return the error message Query too complex: running out of stack space or INSERT INTO: NOT NULL constraint violated for column t1.c1, instead of crashing straightly.\n\nBacktrace\n\n#0 0x7f0c3760dbda (list_append+0x1a)\n#1 0x7f0c376669c6 (rel_insert+0x196)\n#2 0x7f0c3766abb3 (rel_updates+0x1ce3)\n#3 0x7f0c376c6c11 (sequential_block+0x121)\n#4 0x7f0c376c52dc (rel_psm+0x131c)\n#5 0x7f0c37653091 (rel_semantic+0x91)\n#6 0x7f0c37652e6a (rel_parse+0x19a)\n#7 0x7f0c37563102 (sql_insert_triggers+0x232)\n#8 0x7f0c3755bb5e (rel2bin_insert+0x148e)\n#9 0x7f0c37554539 (subrel_bin+0xd69)\n#10 0x7f0c3755f218 (exp_bin+0x29e8)\n#11 0x7f0c37558567 (subrel_bin+0x4d97)\n#12 0x7f0c37563162 (sql_insert_triggers+0x292)\n#13 0x7f0c3755bb5e (rel2bin_insert+0x148e)\n#14 0x7f0c37554539 (subrel_bin+0xd69)\n#15 0x7f0c3755f218 (exp_bin+0x29e8)\n#16 0x7f0c37558567 (subrel_bin+0x4d97)\n#17 0x7f0c37563162 (sql_insert_triggers+0x292)\n...\n#7407 0x7f0c37563162 (sql_insert_triggers+0x292)\n#7408 0x7f0c3755bb5e (rel2bin_insert+0x148e)\n#7409 0x7f0c37554539 (subrel_bin+0xd69)\n#7410 0x7f0c3755373b (output_rel_bin+0x6b)\n#7411 0x7f0c3757f9d9 (backend_dumpstmt+0x199)\n#7412 0x7f0c3754a367 (SQLparser+0x5d7)\n#7413 0x7f0c3754987b (SQLengine_+0x59b)\n#7414 0x7f0c37548343 (SQLengine+0x23)\n#7415 0x7f0c378d76cf (runScenario+0x4f)\n#7416 0x7f0c378d816c (MSscheduleClient+0x68c)\n#7417 0x7f0c3797fc2b (doChallenge+0xfb)\n#7418 0x7f0c37ffeba0 (THRstarter+0x100)\n#7419 0x7f0c3806ecc4 (thread_starter+0x34)\n#7420 0x7f0c373e3609 (start_thread+0xd9)\n#7421 0x7f0c37308133 (clone+0x43)\nSoftware versions\n\nMonetDB server version: 11.46.0 (hg id: 63a42c2) (pulled from the master branch)\nMonetDB client version: mclient, version 11.48.0 (hg id: 63a42c2)\nOS and version: ubuntu 20.04\nSelf-installed and compiled. The command line of compilation: CC=clang-12 CXX=clang++-12 cmake /root/monetdb_master -DCMAKE_BUILD_TYPE=RelWithDebInfo\nIssue labeling\nbug\n\nAdditional context\nThe MonetDB here runs in-memory database. The command line of starting MonetDB server is mserver5 --in-memory.\n--- end body ---\n\ncomments :\n--- start comments ---\n\n--- end comments ---"
  },
  {
    "role": "assistant",
    "content": " { \"gpt_description\": \"An issue in the list_append component of MonetDB Server v11.45.17 and v11.46.0 allows attackers to cause a Denial of Service (DoS) via crafted SQL statements.\", \"gpt_confidence\": 5,\n    \"gpt_is_relevant\": true }"
  },
  {
    "role": "user",
    "content": "This is a GitHub Issue\nrepo:MonetDB\nowner: apache\nTitle : Dynamic dataset definition #40039\n\n--- start body ---\nDescription\nThis proposes a new class DatasetAlias to put in attributes that accept dataset values, including outlets, inlets on a task, and schedule on a DAG. A dataset alias is resolved at runtime (when the task executes) to emit an event of a “real” dataset, with association to the alias. Downstreams can depend on either the resolved dataset, or the alias itself.\n\nUse case/motivation\nThis is inspired by #34206, but I want to still record some information on the dynamic resolution. It also allows the task to still be depended on as a whole, without needing to declare a separate dataset.\n\nThe new class has one single argument name that niquely identifies the group, similar to Dataset.uri, but this does not need to follow any format. (Maybe with the same restrictions as dag_id etc.) Alias names share the same namespace as dataset URIs to avoid user confusion. (Not sure how best to enforce this though.)\n\nDatasetAlias(name: str)\n(Note: Better ideas for the class name are welcomed.)\n\nEmitting an event associated to a dataset alias\nEvents are not emitted against an alias; a task emit “normal” dataset events instead, and they are associated to the alias. The task must first declare the alias as an outlet, and use outlet_events to add events to it:\n\n@task(outlets=[DatasetAlias(\"my-task-outputs\")])\ndef my_task(*, ds, outlet_events):\n    outlet_events[\"my-task-outputs\"].add(Dataset(f\"s3://bucket/my-task/{ds}\"))\nThis creates a dataset event against the S3 URI normally, but also associates the event to the alias my-task-outputs. A stub lineage airflow://datasets/my-task-outputs will also be emitted in addition to normal lineage values.\n\nYou can also attach extra information when adding the event with the extra parameter:\n\ndef add(\n    self,\n    dataset: Dataset | str,\n    extra: dict[str, Any] | None = None,\n) -> None:\n    ...\nThis can also be done with Metadata. Note that arguments are arranged a bit differently here:\n\n@task(outlets=[DatasetAlias(\"my-task-outputs\")])\ndef my_task(*, ds):\n    s3_dataset = Dataset(f\"s3://bucket/my-task/{ds}\")\n    yield Metadata(s3_dataset, extra={\"k\": \"v\"}, alias=\"my-task-outputs\")\nIt should also be possible to resolve multiple aliases into one dataset by passing a list of aliases:\n\n@task(outlets=[DatasetAlias(\"out1\"), DatasetAlias(\"out2\")])\ndef my_task(*, ds):\n    s3_dataset = Dataset(f\"s3://bucket/my-task/{ds}\")\n    yield Metadata(s3_dataset, extra={\"k\": \"v\"}, alias=[\"out1\", \"out2\"])\nOnly one dataset event is emitted for an added dataset, even if it is added to the alias multiple times, or added to multiple aliases. Extra information between each addition is merged.\n\nAdding an event to multiple aliases\nA problem arises when multiple aliases are resolved into the same dataset, but with different event extra information. An example:\n\n@task(outlets=[DatasetAlias(\"out1\"), DatasetAlias(\"out2\")])\ndef my_task(*, ds, outlet_events):\n    dataset = Dataset(f\"s3://bucket/my-task/{ds}\")\n    outlet_events[\"out1\"].add(dataset, extra={\"foo\": 1})\n    outlet_events[\"out2\"].add(dataset, extra={\"foo\": 2})\nAs mentioned above, only one dataset event is generally emitted from one dataset, even if it is associated with multiple aliases. This is not possible when each alias requires a different extra. Here, two dataset events will be emitted, one with extra {\"foo\": 1} and associated with alias out1, the other with {\"foo\": 2} and associated with out2.\n\nDepending on a dataset alias\nSince events added to an alias are just simple events, a downstream depending on the actual dataset can read events of it normally, without considering the associated aliases. For example:\n\nDAG(..., schedule=Dataset(\"s3://bucket/my-task/2024-05-30\"))\nThis DAG can be triggered by a task with outlets=DatasetAlias(\"xxx\") if and only if the alias is resolved into Dataset(\"s3://bucket/my-task/2024-05-30\").\n\nA downstream can also depend on a dataset alias, both to use it for scheduling, or as an inlet data source. In either case, the authoring syntax is straightforward: Reference the DatasetAlias by name, and the associated events are picked up for scheduling:\n\nDAG(..., schedule=DatasetAlias(\"out\"))\nThe DAG runs whenever a task with outlet DatasetAlias(\"out\") gets associated with at least one dataset at runtime, regardless of the dataset’s identity. The downstream DAG is not triggered if no datasets are associated to the alias for a particular given task run. This also means we can do conditional dataset-triggering!\n\nDatabase changes\nA new table will be added to store the aliases similar to datasets. When an alias is resolved, the corresponding dataset event (and the dataset, if needed) is created. A through-table will be added so dataset events and dataset aliases have a many-to-many relationship to track what aliases are associated to a given event.\n\nUI and API considerations\nSince it is not possible to know what an alias resolves into at DAG-parsing time, we can’t connect an upstream declaring a DatasetAlias outlet to a downstream declaring dependency on a “real” dataset. (It is of course not a problem if the downstream depends on the alias itself.) Do we want to do something with this in the Datasets View?\n\nIt is possible to look into the upstream’s history to know which datasets it has emitted previously, but that could be potentially resource-intensive to fetch the information. It is also unclear how the information can be best displayed; simply adding lines for all past events can be unclear (and confusing with the non-alias dependencies). Not sure how this can be best handled. I’m leaning toward just not bothering with this for now, and discuss with users about possible presentations later after we have real-world use cases.\n\nI have not thought this through, to be honest. We’ll need to figure this out as we go.\n\nRelated issues\nSee also #34206.\n\nAre you willing to submit a PR?\n Yes I am willing to submit a PR!\nCode of Conduct\n I agree to follow this project's Code of Conduct\n\n--- end body ---\n\ncomments :\n--- start comments ---\n\n--- end comments ---"
  },
  {
    "role": "assistant",
    "content": " { \"gpt_description\": \"None\", \"gpt_confidence\": 1,  \"gpt_is_relevant\": false }"
  }
]