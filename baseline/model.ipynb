{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d286137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get parent directory (Thesis-Edvin)\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp/cwe_output.json\", \"r\") as file:\n",
    "    file_id = file.fileno()\n",
    "    print(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"tmp/view_CWE-1000_all_weaknesses.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "cwes = data[\"Weaknesses\"]\n",
    "print(len(cwes))\n",
    "cwes = [w for w in cwes if w[\"MappingNotes\"][\"Usage\"] != \"Prohibited\"]\n",
    "print(len(cwes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025326e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class ReplySchema(BaseModel):\n",
    "    gpt_cwe: str = Field(\n",
    "        description=\"The CWE-ID (number) of the CWE entry that best fits the vulnerability description if any; otherwise, write None\"\n",
    "    )\n",
    "    gpt_cwe_confidence: int = Field(\n",
    "        description=\"An integer from 1 to 5 indicating your level of confidence  (1 = very low, 2 = low, 3 = medium, 4 = high, 5 = very high).\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-40-mini\",\n",
    "    temperature=0.0,\n",
    "    api_key=OPENAI_API_KEY_KTH,  # <- this overrides the default\n",
    ")  # maybe set max_token to 14000\n",
    "\n",
    "prompts_dict = load_prompts(os.getcwd() + \"/../utils/prompts\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", prompts_dict[\"baseline_system_setup\"]), (\"human\", \"{desc}\")],\n",
    ")\n",
    "\n",
    "\n",
    "def parser(message: ReplySchema):\n",
    "    return message.model_dump_json()\n",
    "\n",
    "\n",
    "llm = llm.with_structured_output(ReplySchema)\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from pandarallel import pandarallel\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "from openai import OpenAIError, RateLimitError  # Explicitly import errors\n",
    "\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),  # Retry up to 5 times\n",
    "    wait=wait_exponential(multiplier=2, min=1, max=60),  # Exponential backoff\n",
    "    retry=retry_if_exception_type(RateLimitError),  # Retry only on rate limit errors\n",
    ")\n",
    "def _gpt_classify(desc, cwe_entries):\n",
    "    if (\n",
    "        not desc\n",
    "        or not isinstance(desc, str)\n",
    "        or not cwe_entries\n",
    "        or not isinstance(cwe_entries, str)\n",
    "    ):  # Check for empty/invalid messages\n",
    "        return None\n",
    "    return chain.invoke(\n",
    "        {\n",
    "            \"cwe_entries\": cwe_entries,\n",
    "            \"desc\": desc,\n",
    "        }\n",
    "    )  # Adjusted for OpenAI API format\n",
    "\n",
    "\n",
    "def gpt_classify(desc, cwe_entries):\n",
    "    try:\n",
    "        return _gpt_classify(desc, cwe_entries)\n",
    "    except OpenAIError as e:  # Catch all OpenAI-specific errors\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"General error processing message: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcccd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cwe_list_to_json(cwes, indent=4):\n",
    "    return json.dumps({\"Weaknesses\": cwes}, indent=indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-4\", echo=False) -> int:\n",
    "    \"\"\"Count tokens for OpenAI models using tiktoken\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        if echo:\n",
    "            print(\"Finished encoding using model:\", model)\n",
    "    except KeyError:\n",
    "        print(\"invalid input model:\", model + \".\", \"Defaulting to cl100k_base\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")  # Fallback for most models\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd612cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "\n",
    "cwe_token_counts = [count_tokens(json.dumps(cwe, indent=4)) + 2 for cwe in cwes]\n",
    "print(cwe_token_counts)\n",
    "print(\"Mean token count:\\t\\t\", stat.mean(cwe_token_counts))\n",
    "print(\"Median token count:\\t\\t\", stat.median(cwe_token_counts))\n",
    "print(\"Max token count:\\t\\t\", max(cwe_token_counts))\n",
    "print(\"Min token count:\\t\\t\", min(cwe_token_counts))\n",
    "print(\"Total token count:\\t\\t\", sum(cwe_token_counts))\n",
    "print(\"Total packaged token count:\\t\", count_tokens(cwe_list_to_json(cwes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04416114",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwe_chunk_json = cwe_list_to_json(cwes[130:210])\n",
    "count_tokens(cwe_chunk_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_few = load_dataset(\n",
    "    \"Eathus/github-issues-vul-detection-gpt-few-vul-desc-results\", split=\"test\"\n",
    ")\n",
    "test_few_df = test_few.to_pandas()\n",
    "print(\"all cves:\\t\\t\\t\", len(test_few_df))\n",
    "test_few_df = test_few_df[~test_few_df.duplicated(subset=\"issue_github_id\", keep=False)]\n",
    "print(\"non duplicate issues cves:\\t\", len(test_few_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3671d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_few = test_few_df[test_few_df.gpt_is_relevant & ~test_few_df.cve_id.isna()]\n",
    "false_pos_few = test_few_df[test_few_df.gpt_is_relevant & test_few_df.cve_id.isna()]\n",
    "all_true_few = test_few_df[test_few_df.gpt_is_relevant]\n",
    "\n",
    "print(\"true pssetive count:\\t\", len(true_pos_few))\n",
    "print(\"false posetive count:\\t\", len(false_pos_few))\n",
    "print(\"TP + FP count:\\t\\t\", len(all_true_few))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_few_df))\n",
    "print(len(test_few_df.drop_duplicates(subset=\"issue_github_id\")))\n",
    "print(len(test_few_df.drop_duplicates(subset=[\"issue_github_id\", \"cve_id\"])))\n",
    "\n",
    "print(len(true_pos_few))\n",
    "print(len(true_pos_few.drop_duplicates(subset=\"issue_github_id\")))\n",
    "print(len(true_pos_few.drop_duplicates(subset=[\"issue_github_id\", \"cve_id\"])))\n",
    "\n",
    "dupes = test_few_df[\n",
    "    test_few_df.duplicated(subset=\"issue_github_id\", keep=False)\n",
    "].sort_values(\"issue_github_id\")\n",
    "print(len(dupes))\n",
    "display(dupes[[\"cve_id\", \"issue_github_id\", \"issue_number\", \"cve_primary_cwe\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5704f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_pos_few.iloc[0].gpt_description)\n",
    "print(true_pos_few.iloc[0].cve_primary_cwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2189cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def count_chat_tokens(messages, model=\"gpt-4\", echo=False):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        if echo:\n",
    "            print(\"Finished encoding using model:\", model)\n",
    "\n",
    "    except KeyError:\n",
    "        print(\"invalid input model:\", model + \".\", \"Defaulting to cl100k_base\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "\n",
    "    total_tokens = 0\n",
    "    for msg in messages:\n",
    "        total_tokens += tokens_per_message\n",
    "        for key, value in msg.items():\n",
    "            total_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                total_tokens += tokens_per_name\n",
    "    total_tokens += 3  # priming\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc28838",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "    cwe_entries=cwe_chunk_json\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": true_pos_few.iloc[0].gpt_description},\n",
    "]\n",
    "\n",
    "print(count_chat_tokens(messages))\n",
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(cwe_entries=\"\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": true_pos_few.iloc[0].gpt_description},\n",
    "]\n",
    "print(count_chat_tokens(messages))\n",
    "print(count_tokens(cwe_chunk_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ab4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_few.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import *\n",
    "import time\n",
    "\n",
    "\n",
    "def find_cwe_list(high, low, cwes, max_request_size, cwe_tc_list, msg_tc):\n",
    "    # Recursive binary search function\n",
    "    mid = low + (high - low) // 2\n",
    "    token_count = sum(cwe_tc_list[:mid]) + msg_tc + 11\n",
    "\n",
    "    if low == high:\n",
    "        return [], 0, token_count\n",
    "\n",
    "    if token_count == max_request_size:\n",
    "        return cwes[:mid], mid, token_count\n",
    "    if high - low == 1:\n",
    "        max_tc = token_count + cwe_tc_list[mid]\n",
    "        if max_tc < max_request_size:\n",
    "            return cwes[:high], high, max_tc\n",
    "        return cwes[:mid], mid, token_count\n",
    "\n",
    "    if token_count < max_request_size:\n",
    "        return find_cwe_list(high, mid, cwes, max_request_size, cwe_tc_list, msg_tc)\n",
    "    else:\n",
    "        return find_cwe_list(mid, low, cwes, max_request_size, cwe_tc_list, msg_tc)\n",
    "\n",
    "\n",
    "def get_prompt_preamble_tc(desc, prompts_dict, ignore_user=False):\n",
    "    formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "        cwe_entries=\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": (\"\" if ignore_user else desc)},\n",
    "    ]\n",
    "    return count_chat_tokens(messages)\n",
    "\n",
    "\n",
    "def filter_possible_cwes(cwes):\n",
    "    possible_cwes = [resp for resp in cwes if resp[\"gpt_cwe\"] != \"None\"]\n",
    "\n",
    "    if possible_cwes == []:\n",
    "        return [min(cwes, key=(lambda x: x[\"gpt_cwe_confidence\"]))]\n",
    "\n",
    "    max_conf = max(possible_cwes, key=(lambda x: x[\"gpt_cwe_confidence\"]))[\n",
    "        \"gpt_cwe_confidence\"\n",
    "    ]\n",
    "    possible_cwes = [\n",
    "        resp for resp in possible_cwes if resp[\"gpt_cwe_confidence\"] == max_conf\n",
    "    ]\n",
    "    possible_cwes = list({p[\"gpt_cwe\"]: p for p in possible_cwes}.values())\n",
    "    return possible_cwes\n",
    "\n",
    "\n",
    "def get_gpt_cwe(cwes, desc, max_request_size, echo=False):\n",
    "    prompts_dict = load_prompts(os.getcwd() + \"/../utils/prompts\")\n",
    "    cwe_dict = {cwe[\"ID\"]: cwe for cwe in cwes}\n",
    "    msg_tc = get_prompt_preamble_tc(desc, prompts_dict)\n",
    "\n",
    "    while True:\n",
    "        i = 0\n",
    "        responses = []\n",
    "        # print(\"cwes:\\n\", len(cwes))\n",
    "        inner_pbar = tqdm(\n",
    "            total=len(cwes),\n",
    "            desc=\"Processing CWE chunks\",\n",
    "            leave=False,\n",
    "            disable=not echo,\n",
    "        )\n",
    "\n",
    "        while i < len(cwes):\n",
    "            # print(\"i:\", i)\n",
    "            cwe_chunk, next_i, _ = find_cwe_list(\n",
    "                len(cwes[i:]),\n",
    "                0,\n",
    "                cwes[i:],\n",
    "                max_request_size,\n",
    "                cwe_token_counts[i:],\n",
    "                msg_tc,\n",
    "            )\n",
    "            # print(any(cwe['ID'] == '415' for cwe in cwe_chunk))\n",
    "            i += next_i\n",
    "\n",
    "            response = '{\"gpt_cwe\":\"415\",\"gpt_cwe_confidence\":5}'\n",
    "            time.sleep(0.5)\n",
    "            # response = gpt_classify(desc, cwe_list_to_json(cwe_chunk))\n",
    "\n",
    "            response = json.loads(response)\n",
    "            # print(response)\n",
    "            responses.append(response)\n",
    "\n",
    "            inner_pbar.update(next_i)\n",
    "\n",
    "        inner_pbar.close()\n",
    "\n",
    "        poss_cwes = filter_possible_cwes(responses)\n",
    "        if len(poss_cwes) == 1:\n",
    "            return poss_cwes[0]\n",
    "\n",
    "        cwes = [cwe_dict[cwe[\"gpt_cwe\"]] for cwe in poss_cwes]\n",
    "\n",
    "\n",
    "def classify_issues(cwes, data_df, max_request_size, echo=False):\n",
    "    issue_dict = {\n",
    "        dat.issue_github_id: ([], dat.gpt_description)\n",
    "        for (_, dat) in data_df.iterrows()\n",
    "    }\n",
    "    cwe_dict = {cwe[\"ID\"]: cwe for cwe in cwes}\n",
    "\n",
    "    prompts_dict = load_prompts(os.getcwd() + \"/../utils/prompts\")\n",
    "    msg_tc = get_prompt_preamble_tc(\"\", prompts_dict, True)\n",
    "\n",
    "    i = 0\n",
    "    req_number = 0\n",
    "    chunks = []\n",
    "    while i < len(cwes):\n",
    "        # print(\"i:\", i)\n",
    "        cwe_chunk, next_i, _ = find_cwe_list(\n",
    "            len(cwes[i:]),\n",
    "            0,\n",
    "            cwes[i:],\n",
    "            max_request_size,\n",
    "            cwe_token_counts[i:],\n",
    "            msg_tc,\n",
    "        )\n",
    "        i += next_i\n",
    "\n",
    "        chunks.append(cwe_chunk)\n",
    "        req_number += 1\n",
    "\n",
    "    chunk_iter = tqdm(chunks, desc=\"Processing CWE chunks\", disable=not echo)\n",
    "    for chunk in chunk_iter:\n",
    "        data_iter = tqdm(\n",
    "            data_df.iterrows(),\n",
    "            total=len(data_df),\n",
    "            desc=\"Classifying issues\",\n",
    "            leave=False,\n",
    "            disable=not echo,\n",
    "        )\n",
    "        for _, dat in data_iter:\n",
    "            # response = gpt_classify(dat.gpt_description, cwe_list_to_json(chunk))\n",
    "            # issue_dict[dat.issue_github_id][0].append(response)\n",
    "\n",
    "            time.sleep(0.001)\n",
    "            issue_dict[dat.issue_github_id][0].extend(\n",
    "                [\n",
    "                    {\"gpt_cwe\": \"415\", \"gpt_cwe_confidence\": 5},\n",
    "                    {\"gpt_cwe\": \"664\", \"gpt_cwe_confidence\": 5},\n",
    "                    {\"gpt_cwe\": \"666\", \"gpt_cwe_confidence\": 5},\n",
    "                    {\"gpt_cwe\": \"416\", \"gpt_cwe_confidence\": 5},\n",
    "                    {\"gpt_cwe\": \"1341\", \"gpt_cwe_confidence\": 5},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    data_iter = tqdm(\n",
    "        issue_dict.items(),\n",
    "        desc=\"Re-classifying issues\",\n",
    "        disable=not echo,\n",
    "        leave=False,\n",
    "    )\n",
    "    for key, val in data_iter:\n",
    "        poss_cwes = filter_possible_cwes(val[0])\n",
    "        if len(poss_cwes) == 1:\n",
    "            issue_dict[key] = poss_cwes[0]\n",
    "        else:\n",
    "            remaining_cwes = [cwe_dict[cwe[\"gpt_cwe\"]] for cwe in poss_cwes]\n",
    "            issue_dict[key] = get_gpt_cwe(\n",
    "                remaining_cwes, val[1], max_request_size, echo\n",
    "            )\n",
    "\n",
    "    return issue_dict\n",
    "\n",
    "\n",
    "def batch_cwe_request(cwes, desc, git_issue_id, max_request_size, model=\"gpt-40-mini\", echo=True):\n",
    "    prompts_dict = load_prompts(os.getcwd() + \"/../utils/prompts\")\n",
    "    msg_tc = get_prompt_preamble_tc(desc, prompts_dict)\n",
    "\n",
    "    i = 0\n",
    "    req_number = 0\n",
    "    requests = []\n",
    "\n",
    "    inner_pbar = tqdm(\n",
    "        total=len(cwes),\n",
    "        desc=f\"Gathering CWE requests for issue {git_issue_id}\",\n",
    "        leave=False,\n",
    "        disable=not echo,\n",
    "    )\n",
    "    while i < len(cwes):\n",
    "        # print(\"i:\", i)\n",
    "        cwe_chunk, next_i, _ = find_cwe_list(\n",
    "            len(cwes[i:]),\n",
    "            0,\n",
    "            cwes[i:],\n",
    "            max_request_size,\n",
    "            cwe_token_counts[i:],\n",
    "            msg_tc,\n",
    "        )\n",
    "        i += next_i\n",
    "\n",
    "        formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "            cwe_entries=cwe_list_to_json(cwe_chunk)\n",
    "        )\n",
    "        request = {\n",
    "            \"custom_id\": str(git_issue_id) + \"-\" + str(req_number),\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": desc},\n",
    "                ],\n",
    "                \"max_tokens\": 1000,\n",
    "            },\n",
    "        }\n",
    "        requests.append(request)\n",
    "        req_number += 1\n",
    "\n",
    "\n",
    "        inner_pbar.update(next_i)\n",
    "\n",
    "    inner_pbar.close()\n",
    "\n",
    "    return requests\n",
    "\n",
    "\n",
    "def generate_batch_post_files(\n",
    "    cwes,\n",
    "    data_df,\n",
    "    max_request_size,\n",
    "    post_folder_path='tmp',\n",
    "    max_size_mb=150,\n",
    "    output_dir=\"batches\",\n",
    "    output_prefix=\"batch\",\n",
    "    echo=True\n",
    "):\n",
    "    output_dir = os.path.join(post_folder_path, output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    #requests = []\n",
    "    batch_info_dict = {}\n",
    "\n",
    "    max_size_bytes = max_size_mb * 1024 * 1024\n",
    "    part_num = 1\n",
    "    current_size = 0\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"{output_prefix}_{part_num}.jsonl\")\n",
    "    output_file = open(output_path, 'w', encoding='utf-8')\n",
    "\n",
    "    data_iter = tqdm(\n",
    "        data_df.iterrows(),\n",
    "        total=len(data_df),\n",
    "        desc=\"Writing batch files\",\n",
    "        disable=not echo,\n",
    "    )\n",
    "    for _, row in data_iter:\n",
    "        requests = batch_cwe_request(\n",
    "            cwes, row.gpt_description, row.issue_github_id, max_request_size\n",
    "        ) \n",
    "        req_iter = tqdm(\n",
    "            requests,\n",
    "            desc=f\"Writing issue {row.issue_github_id} batch files\",\n",
    "            leave=False,\n",
    "            disable=not echo,\n",
    "        )\n",
    "        for req in req_iter:\n",
    "            req_line = json.dumps(req) + \"\\n\"\n",
    "            line_size = len(req_line.encode('utf-8'))\n",
    "            if current_size + line_size > max_size_bytes:\n",
    "                    output_file.close()\n",
    "                    part_num += 1\n",
    "                    output_path = os.path.join(output_dir, f\"{output_prefix}_{part_num}.jsonl\")\n",
    "                    output_file = open(output_path, 'w', encoding='utf-8')\n",
    "                    current_size = 0\n",
    "            output_file.write(req_line)\n",
    "            current_size += line_size\n",
    "        del requests\n",
    "\n",
    "    output_file.close()\n",
    "    batch_info_dict['batch_count'] = part_num\n",
    "    batch_info_dict['batch_prefix'] = output_prefix\n",
    "    batch_info_dict['batch_size'] = max_size_mb\n",
    "    batch_info_dict['batch_issues'] = data_df['issue_github_id'].tolist()\n",
    "\n",
    "    pickle_file = os.path.join(output_dir, 'batch_info.pkl')\n",
    "    with open(pickle_file, 'wb') as file:\n",
    "        pickle.dump(batch_info_dict, file)\n",
    "\n",
    "def post_batches(\n",
    "    post_folder_path='tmp',\n",
    "    batches_dir=\"batches\",\n",
    ") :\n",
    "    batches_dir = os.path.join(post_folder_path, batches_dir)\n",
    "    pickle_file = os.path.join(batches_dir, 'batch_info.pkl')\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        batch_info_dict = pickle.load(file)\n",
    "    \n",
    "    #files = []\n",
    "    batches = []\n",
    "    for i in range(1, batch_info_dict['batch_count'] + 1) :\n",
    "        path = os.path.join(batches_dir, f\"{batch_info_dict['batch_prefix']}_{i}.jsonl\")\n",
    "        file = client.files.create(\n",
    "            file=open(path, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        #files.append(file)\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=file.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "                \"batch_name\": f\"{batch_info_dict['batch_prefix']}_{i}\"\n",
    "            }\n",
    "        )\n",
    "        batches.append(batch['id'])\n",
    "    \n",
    "    batch_info_dict['batch_ids'] = batches\n",
    "    with open(pickle_file, 'wb') as file:\n",
    "        pickle.dump(batch_info_dict, file)\n",
    "\n",
    "def retrieve_batch_res(\n",
    "    post_folder_path='tmp',\n",
    "    batches_dir=\"batches\",\n",
    "    output_dir='responses'\n",
    "\n",
    ") :\n",
    "    batches_dir = os.path.join(post_folder_path, batches_dir)\n",
    "    output_dir = os.path.join(post_folder_path, output_dir)\n",
    "\n",
    "    pickle_file = os.path.join(batches_dir, 'batch_info.pkl')\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        batch_info_dict = pickle.load(file)\n",
    "    \n",
    "    statuses = {}\n",
    "    for batch_id in batch_info_dict['batch_ids'] :\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        status = batch['status'] \n",
    "        statuses[batch['metadata']['batch_name']] = status\n",
    "        if status == 'completed' :\n",
    "            file_response = client.files.content(batch['id'])\n",
    "            output_file = os.path.join(output_dir, batch['metadata']['batch_name'] + 'response')\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(file_response.text)\n",
    "            batch_info_dict['batch_ids'].remove(batch_id)\n",
    "        else :\n",
    "            print('batch,', batch['metadata']['batch_name'], 'status:\\t', status)\n",
    "    \n",
    "    return statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c67bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(true_pos_few['issue_github_id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6faff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_batch_post_files(cwes, true_pos_few, 100000, max_size_mb=190)\n",
    "# classify_issues(cwes, true_pos_few, 100000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77692db",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"custom_id\": \"request-1\",\n",
    "    \"method\": \"POST\",\n",
    "    \"url\": \"/v1/chat/completions\",\n",
    "    \"body\": {\n",
    "        \"model\": \"gpt-3.5-turbo-0125\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello world!\"},\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "    },\n",
    "}\n",
    "{\n",
    "    \"custom_id\": \"request-2\",\n",
    "    \"method\": \"POST\",\n",
    "    \"url\": \"/v1/chat/completions\",\n",
    "    \"body\": {\n",
    "        \"model\": \"gpt-3.5-turbo-0125\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello world!\"},\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(cwe_entries=\"\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"\"},\n",
    "]\n",
    "msg_tc = count_chat_tokens(messages)\n",
    "cwe_chunk, end_ind, size = find_cwe_list(\n",
    "    len(cwes[186:]), 0, cwes[186:], 100000, cwe_token_counts[186:], msg_tc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "    cwe_entries=cwe_list_to_json(cwe_chunk)\n",
    ")\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": true_pos_few.iloc[0].gpt_description},\n",
    "]\n",
    "print(\"end_ind:\\t\", end_ind)\n",
    "print(\"true size:\\t\", count_chat_tokens(msgs))\n",
    "print(\"size:\\t\", size)\n",
    "print(\"len(cwe_chunk):\\t\", len(cwe_chunk))\n",
    "print(\"cwe_chunk:\\t\", cwe_list_to_json(cwe_chunk))\n",
    "# display(cwe_list_to_json(cwe_chunk, 4))\n",
    "\n",
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "    cwe_entries=cwe_list_to_json(cwe_chunk)\n",
    ")\n",
    "display(formatted_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26941647",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cwes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e731b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxr_size = 100000\n",
    "resp = get_gpt_cwe(cwes, true_pos_few.iloc[0].gpt_description, maxr_size, echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(resp))\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mthesis_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
