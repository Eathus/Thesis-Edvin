{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d286137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get parent directory (Thesis-Edvin)\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e4b52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "with open(\"tmp/cwe_output.json\", \"r\") as file:\n",
    "    file_id = file.fileno()\n",
    "    print(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b08e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940\n",
      "881\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"tmp/view_CWE-1000_all_weaknesses.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "cwes = data[\"Weaknesses\"]\n",
    "print(len(cwes))\n",
    "cwes = [w for w in cwes if w[\"MappingNotes\"][\"Usage\"] != \"Prohibited\"]\n",
    "print(len(cwes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025326e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class ReplySchema(BaseModel):\n",
    "    gpt_cwe: str = Field(\n",
    "        description=\"The CWE-ID (number) of the CWE entry that best fits the vulnerability description if any; otherwise, write None\"\n",
    "    )\n",
    "    gpt_cwe_confidence: int = Field(\n",
    "        description=\"An integer from 1 to 5 indicating your level of confidence  (1 = very low, 2 = low, 3 = medium, 4 = high, 5 = very high).\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    api_key=OPENAI_API_KEY_KTH,  # <- this overrides the default\n",
    ")  # maybe set max_token to 14000\n",
    "\n",
    "prompts_dict = load_prompts(os.getcwd() + \"/../utils/prompts\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", prompts_dict[\"baseline_system_setup\"]), (\"human\", \"{desc}\")],\n",
    ")\n",
    "\n",
    "\n",
    "def parser(message: ReplySchema):\n",
    "    return message.model_dump_json()\n",
    "\n",
    "\n",
    "llm = llm.with_structured_output(ReplySchema)\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02a0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from pandarallel import pandarallel\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "from openai import OpenAIError, RateLimitError  # Explicitly import errors\n",
    "\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),  # Retry up to 5 times\n",
    "    wait=wait_exponential(multiplier=2, min=1, max=60),  # Exponential backoff\n",
    "    retry=retry_if_exception_type(RateLimitError),  # Retry only on rate limit errors\n",
    ")\n",
    "def _gpt_classify(desc, cwe_entries):\n",
    "    if (\n",
    "        not desc\n",
    "        or not isinstance(desc, str)\n",
    "        or not cwe_entries\n",
    "        or not isinstance(cwe_entries, str)\n",
    "    ):  # Check for empty/invalid messages\n",
    "        return None\n",
    "    return chain.invoke(\n",
    "        {\n",
    "            \"cwe_entries\": cwe_entries,\n",
    "            \"desc\": desc,\n",
    "        }\n",
    "    )  # Adjusted for OpenAI API format\n",
    "\n",
    "\n",
    "def gpt_classify(desc, cwe_entries):\n",
    "    try:\n",
    "        return _gpt_classify(desc, cwe_entries)\n",
    "    except OpenAIError as e:  # Catch all OpenAI-specific errors\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"General error processing message: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcccd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cwe_list_to_json(cwes, indent=4):\n",
    "    return json.dumps({\"Weaknesses\": cwes}, indent=indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5f4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-4\", echo=False) -> int:\n",
    "    \"\"\"Count tokens for OpenAI models using tiktoken\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        if echo:\n",
    "            print(\"Finished encoding using model:\", model)\n",
    "    except KeyError:\n",
    "        print(\"invalid input model:\", model + \".\", \"Defaulting to cl100k_base\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")  # Fallback for most models\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd612cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4737, 5705, 3561, 1596, 1860, 3281, 2473, 2149, 1636, 2307, 2259, 4172, 3826, 1327, 2799, 2229, 2850, 1906, 2076, 2465, 1607, 1461, 2451, 2022, 1218, 2103, 1994, 1832, 1919, 1517, 1608, 1624, 2055, 3196, 1543, 1552, 1100, 5818, 8131, 2736, 2523, 1233, 1156, 1636, 3391, 2017, 2461, 2337, 3157, 3739, 3023, 1891, 2167, 1985, 2006, 1781, 8077, 677, 1832, 1632, 3487, 2197, 1585, 3396, 1832, 2161, 1349, 1374, 1491, 1203, 1479, 1603, 1108, 10124, 2431, 3761, 1398, 1303, 1939, 1461, 2852, 7058, 1832, 3334, 2265, 1668, 2359, 6271, 3646, 1346, 3242, 2128, 2122, 1471, 3240, 2413, 2901, 1489, 1121, 8522, 1583, 3543, 1614, 2098, 1028, 2731, 2149, 8835, 6391, 3928, 2743, 2714, 2299, 3062, 2292, 2002, 4053, 2039, 2303, 2490, 1338, 1806, 1405, 4772, 2198, 2441, 4628, 3786, 2634, 2538, 1998, 1346, 2440, 1620, 1434, 1794, 2037, 2219, 2773, 1306, 5434, 3703, 5037, 2201, 3187, 2083, 1970, 7561, 3135, 2769, 2138, 1609, 3926, 2198, 2548, 1807, 4096, 1831, 1972, 2563, 2387, 3220, 1293, 2587, 1631, 2082, 1811, 2423, 902, 1730, 2294, 3289, 2300, 1304, 3167, 6312, 1454, 1363, 5563, 1936, 1635, 2241, 1326, 2545, 1711, 11447, 10762, 2233, 5537, 3459, 1829, 1770, 2948, 3654, 9751, 5500, 6012, 3436, 4338, 8384, 4377, 2460, 2950, 2864, 2373, 4386, 6706, 674, 2176, 2424, 1897, 1509, 1803, 2040, 3531, 2580, 2820, 1648, 1297, 2222, 3607, 1812, 1594, 2659, 2743, 4346, 3251, 2381, 1442, 9503, 1675, 1993, 4330, 896, 915, 908, 908, 932, 880, 2480, 3156, 3289, 1733, 1865, 3164, 8773, 1660, 1583, 4718, 1855, 1450, 1647, 1214, 5652, 1283, 2926, 5578, 3095, 1954, 1811, 2022, 2403, 2365, 2430, 1642, 4101, 3351, 3063, 3032, 4842, 903, 4667, 2021, 2512, 2356, 4091, 1715, 1366, 1615, 2234, 1773, 1808, 1803, 2639, 3027, 3353, 4146, 3336, 3761, 6315, 4148, 4976, 3083, 3443, 992, 2198, 3059, 1580, 5677, 2639, 2663, 3116, 3671, 1595, 1118, 2251, 1446, 1340, 982, 2228, 1817, 1004, 2118, 2539, 1250, 860, 1745, 1836, 1842, 5896, 1637, 4049, 925, 1492, 1863, 4397, 2619, 1573, 3562, 4809, 2613, 1533, 5275, 1894, 1894, 3123, 2030, 2004, 1041, 1299, 2485, 2271, 2040, 1427, 1639, 2479, 6197, 8073, 2846, 1686, 1614, 2822, 1741, 5341, 6297, 2879, 3202, 1718, 1673, 1542, 3610, 1666, 6875, 2191, 3250, 1231, 1513, 2456, 1236, 2379, 2422, 5088, 3643, 2220, 1921, 2256, 3407, 1940, 1442, 1618, 1877, 1762, 13307, 7895, 2247, 2171, 2092, 2191, 2567, 2346, 2270, 2455, 2868, 2580, 2552, 2516, 4711, 2536, 2379, 2982, 2190, 3017, 2058, 3614, 2283, 2875, 1455, 2341, 1619, 7707, 2811, 2434, 1970, 6277, 2388, 2373, 2143, 1966, 1463, 1550, 1737, 1878, 1606, 5910, 5756, 2508, 2736, 1738, 1626, 3556, 6544, 7110, 4658, 5733, 3504, 2275, 8263, 5846, 5987, 1498, 1722, 7246, 1535, 3333, 3038, 1481, 3572, 3398, 3494, 4159, 2164, 4008, 3373, 1489, 4102, 1444, 6477, 1581, 1320, 1275, 1360, 1951, 1195, 1503, 1765, 1968, 1401, 1467, 1367, 1586, 1737, 1481, 1766, 1903, 5862, 2179, 3138, 2666, 2410, 1904, 2095, 3064, 2154, 2004, 2095, 1607, 6063, 1881, 2244, 2132, 5229, 2388, 8129, 2332, 6595, 3524, 1524, 1560, 7182, 2377, 1429, 2037, 1782, 1494, 1496, 2279, 1076, 4796, 2454, 10668, 3332, 2061, 3647, 3186, 10036, 1645, 2874, 5711, 3405, 2307, 2697, 2784, 2604, 3419, 9417, 1634, 3393, 4648, 1304, 3612, 2181, 2965, 5263, 2020, 1826, 1869, 1860, 1773, 2662, 4340, 3262, 2408, 3437, 2398, 2852, 2559, 1983, 2651, 2897, 1669, 967, 3064, 2687, 2131, 2516, 2883, 1865, 2160, 2319, 1408, 1822, 1766, 1275, 2726, 857, 3032, 3976, 3397, 2437, 1739, 1532, 1493, 1903, 1719, 1443, 2131, 2894, 2887, 1730, 2898, 4392, 1635, 7914, 6632, 1606, 2066, 2969, 5789, 4803, 5556, 4337, 8439, 5082, 1686, 6982, 3207, 3507, 2062, 2419, 2662, 2003, 2418, 2597, 2720, 2866, 2985, 2502, 2840, 1926, 2457, 2089, 2180, 2313, 1855, 2077, 2931, 1571, 2652, 2147, 1493, 1243, 7387, 2558, 2333, 1766, 2013, 1888, 2262, 996, 1069, 1590, 2046, 4408, 3124, 2840, 2725, 2872, 1879, 5302, 3268, 2013, 2640, 5049, 4540, 868, 1062, 2820, 2742, 975, 2376, 3767, 1486, 3220, 2004, 1787, 3687, 1936, 4425, 2094, 2978, 1698, 2246, 2250, 2437, 2112, 1066, 2087, 1294, 1686, 1719, 3004, 1442, 1357, 990, 1330, 1471, 2024, 5001, 2265, 2385, 7962, 7836, 2591, 2194, 1953, 7535, 2529, 2282, 4735, 2539, 1582, 2621, 1580, 2346, 2131, 2146, 7547, 4066, 2278, 2980, 3269, 2376, 2401, 2303, 2368, 2504, 2458, 2348, 2621, 2025, 2226, 3170, 2591, 2461, 2336, 2509, 2578, 2505, 2326, 3442, 2502, 2402, 2430, 2645, 2466, 2277, 2285, 2101, 1230, 1317, 1382, 1297, 1381, 1854, 1251, 1273, 2261, 4718, 2608, 2174, 2449, 2006, 3010, 3113, 13764, 1186, 2151, 3216, 2798, 1891, 1970, 676, 1495, 1960, 2620, 2822, 10318, 2539, 1482, 2583, 1210, 1107, 1585, 1728, 2018, 6534, 2070, 1406, 1687, 8017, 4488, 1813, 13446, 7898, 2358, 14735, 3215, 2807, 2984, 2435, 2695, 2317, 2607, 2914, 3169, 2245, 3556, 2908, 12717, 2065, 3241, 3886, 1854, 1862, 1695, 1306, 1497, 800, 1017, 1494, 1475, 2053, 845, 845, 843, 982, 1788, 1991, 621, 678, 878, 792, 612, 636, 1230, 2584, 1364, 1913, 2203, 3490, 2061, 1525, 3051, 4201, 1692, 2651, 1000, 3405, 2960, 1974, 2968, 1997, 1427, 2970, 2525, 2538, 1723, 1946, 2056, 1776, 2562, 1605, 1727, 1733, 1896, 1364, 1793, 2193, 3129, 2309, 3139, 2168, 2118, 1975, 2603, 2010, 3287, 1194, 3794, 2283, 1823, 1526, 2122, 1701, 3122, 1584, 1088, 1310, 1278, 2465, 1941, 1261, 1306, 1919, 1799, 1149, 2221, 859, 1899, 1769, 2450]\n",
      "Mean token count:\t\t 2766.0771850170263\n",
      "Median token count:\t\t 2262\n",
      "Max token count:\t\t 14735\n",
      "Min token count:\t\t 612\n",
      "Total token count:\t\t 2436914\n",
      "Total packaged token count:\t 2436925\n"
     ]
    }
   ],
   "source": [
    "import statistics as stat\n",
    "\n",
    "cwe_token_counts = [count_tokens(json.dumps(cwe, indent=4)) + 2 for cwe in cwes]\n",
    "print(cwe_token_counts)\n",
    "print(\"Mean token count:\\t\\t\", stat.mean(cwe_token_counts))\n",
    "print(\"Median token count:\\t\\t\", stat.median(cwe_token_counts))\n",
    "print(\"Max token count:\\t\\t\", max(cwe_token_counts))\n",
    "print(\"Min token count:\\t\\t\", min(cwe_token_counts))\n",
    "print(\"Total token count:\\t\\t\", sum(cwe_token_counts))\n",
    "print(\"Total packaged token count:\\t\", count_tokens(cwe_list_to_json(cwes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04416114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251702"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwe_chunk_json = cwe_list_to_json(cwes[130:210])\n",
    "count_tokens(cwe_chunk_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c48a063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cves:\t\t\t 1778\n",
      "non duplicate issues cves:\t 1763\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_few = load_dataset(\n",
    "    \"Eathus/github-issues-vul-detection-gpt-few-strict-vul-desc-results\", split=\"test\"\n",
    ")\n",
    "test_few_df = test_few.to_pandas()\n",
    "print(\"all cves:\\t\\t\\t\", len(test_few_df))\n",
    "test_few_df = test_few_df[~test_few_df.duplicated(subset=\"issue_github_id\", keep=False)]\n",
    "print(\"non duplicate issues cves:\\t\", len(test_few_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3671d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true pssetive count:\t 291\n",
      "false posetive count:\t 310\n",
      "TP + FP count:\t\t 601\n"
     ]
    }
   ],
   "source": [
    "true_pos_few = test_few_df[test_few_df.gpt_is_relevant & ~test_few_df.cve_id.isna()]\n",
    "false_pos_few = test_few_df[test_few_df.gpt_is_relevant & test_few_df.cve_id.isna()]\n",
    "all_true_few = test_few_df[test_few_df.gpt_is_relevant]\n",
    "\n",
    "print(\"true pssetive count:\\t\", len(true_pos_few))\n",
    "print(\"false posetive count:\\t\", len(false_pos_few))\n",
    "print(\"TP + FP count:\\t\\t\", len(all_true_few))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a0369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763\n",
      "1763\n",
      "1763\n",
      "291\n",
      "291\n",
      "291\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cve_id</th>\n",
       "      <th>issue_github_id</th>\n",
       "      <th>issue_number</th>\n",
       "      <th>cve_primary_cwe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cve_id, issue_github_id, issue_number, cve_primary_cwe]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(test_few_df))\n",
    "print(len(test_few_df.drop_duplicates(subset=\"issue_github_id\")))\n",
    "print(len(test_few_df.drop_duplicates(subset=[\"issue_github_id\", \"cve_id\"])))\n",
    "\n",
    "print(len(true_pos_few))\n",
    "print(len(true_pos_few.drop_duplicates(subset=\"issue_github_id\")))\n",
    "print(len(true_pos_few.drop_duplicates(subset=[\"issue_github_id\", \"cve_id\"])))\n",
    "\n",
    "dupes = test_few_df[\n",
    "    test_few_df.duplicated(subset=\"issue_github_id\", keep=False)\n",
    "].sort_values(\"issue_github_id\")\n",
    "print(len(dupes))\n",
    "display(dupes[[\"cve_id\", \"issue_github_id\", \"issue_number\", \"cve_primary_cwe\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b5704f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue describes a double free vulnerability in the LibreDWG library, specifically in the function `dwg_free_MATERIAL_private` at line 7662 of `dwg.spec`. The AddressSanitizer output indicates that the program attempts to free the same memory address twice, which can lead to undefined behavior, including potential exploitation by attackers.\n",
      "415\n"
     ]
    }
   ],
   "source": [
    "print(true_pos_few.iloc[0].gpt_description)\n",
    "print(true_pos_few.iloc[0].cve_primary_cwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e2189cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def count_chat_tokens(messages, model=\"gpt-4\", echo=False):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        if echo:\n",
    "            print(\"Finished encoding using model:\", model)\n",
    "\n",
    "    except KeyError:\n",
    "        print(\"invalid input model:\", model + \".\", \"Defaulting to cl100k_base\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "\n",
    "    total_tokens = 0\n",
    "    for msg in messages:\n",
    "        total_tokens += tokens_per_message\n",
    "        for key, value in msg.items():\n",
    "            total_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                total_tokens += tokens_per_name\n",
    "    total_tokens += 3  # priming\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dc28838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252024\n",
      "322\n",
      "251702\n"
     ]
    }
   ],
   "source": [
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "    cwe_entries=cwe_chunk_json\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": true_pos_few.iloc[0].gpt_description},\n",
    "]\n",
    "\n",
    "print(count_chat_tokens(messages))\n",
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(cwe_entries=\"\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": true_pos_few.iloc[0].gpt_description},\n",
    "]\n",
    "print(count_chat_tokens(messages))\n",
    "print(count_tokens(cwe_chunk_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "615ab4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cve_id', 'cve_published', 'cve_descriptions', 'cve_metrics',\n",
       "       'cve_references', 'cve_configurations', 'cve_primary_cwe', 'cve_tags',\n",
       "       'issue_owner_repo', 'issue_body', 'issue_title', 'issue_comments_url',\n",
       "       'issue_comments_count', 'issue_created_at', 'issue_updated_at',\n",
       "       'issue_html_url', 'issue_github_id', 'issue_number', 'label',\n",
       "       'issue_msg', 'issue_msg_n_tokens', 'issue_embedding',\n",
       "       '__index_level_0__', 'gpt_description', 'gpt_vulnerability',\n",
       "       'gpt_confidence', 'gpt_is_relevant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos_few.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f92b8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import *\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def find_cwe_list(high, low, cwes, max_request_size, cwe_tc_list, msg_tc):\n",
    "    # Recursive binary search function\n",
    "    mid = low + (high - low) // 2\n",
    "    token_count = sum(cwe_tc_list[:mid]) + msg_tc + 11\n",
    "\n",
    "    if low == high:\n",
    "        return [], 0, token_count\n",
    "\n",
    "    if token_count == max_request_size:\n",
    "        return cwes[:mid], mid, token_count\n",
    "    if high - low == 1:\n",
    "        max_tc = token_count + cwe_tc_list[mid]\n",
    "        if max_tc < max_request_size:\n",
    "            return cwes[:high], high, max_tc\n",
    "        return cwes[:mid], mid, token_count\n",
    "\n",
    "    if token_count < max_request_size:\n",
    "        return find_cwe_list(high, mid, cwes, max_request_size, cwe_tc_list, msg_tc)\n",
    "    else:\n",
    "        return find_cwe_list(mid, low, cwes, max_request_size, cwe_tc_list, msg_tc)\n",
    "\n",
    "\n",
    "def get_prompt_preamble_tc(desc, prompts_dict, ignore_user=False):\n",
    "    formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "        cwe_entries=\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": (\"\" if ignore_user else desc)},\n",
    "    ]\n",
    "    return count_chat_tokens(messages)\n",
    "\n",
    "\n",
    "def filter_possible_cwes(cwes):\n",
    "    possible_cwes = [resp for resp in cwes if resp[\"gpt_cwe\"] != \"None\"]\n",
    "\n",
    "    if possible_cwes == []:\n",
    "        return [min(cwes, key=(lambda x: x[\"gpt_cwe_confidence\"]))] if cwes else [] \n",
    "\n",
    "    max_conf = max(possible_cwes, key=(lambda x: x[\"gpt_cwe_confidence\"]))[\n",
    "        \"gpt_cwe_confidence\"\n",
    "    ]\n",
    "    possible_cwes = [\n",
    "        resp for resp in possible_cwes if resp[\"gpt_cwe_confidence\"] == max_conf\n",
    "    ]\n",
    "    possible_cwes = list({p[\"gpt_cwe\"]: p for p in possible_cwes}.values())\n",
    "    return possible_cwes\n",
    "\n",
    "\n",
    "def get_gpt_cwe(cwes, desc, max_request_size, echo=False):\n",
    "    prompts_dict = load_prompts(os.getcwd() + \"/../utils/prompts\")\n",
    "    cwe_dict = {cwe[\"ID\"]: cwe for cwe in cwes}\n",
    "    msg_tc = get_prompt_preamble_tc(desc, prompts_dict)\n",
    "\n",
    "    while True:\n",
    "        i = 0\n",
    "        responses = []\n",
    "        # print(\"cwes:\\n\", len(cwes))\n",
    "        inner_pbar = tqdm(\n",
    "            total=len(cwes),\n",
    "            desc=\"Processing CWE chunks\",\n",
    "            leave=False,\n",
    "            disable=not echo,\n",
    "        )\n",
    "\n",
    "        while i < len(cwes):\n",
    "            # print(\"i:\", i)\n",
    "            cwe_chunk, next_i, _ = find_cwe_list(\n",
    "                len(cwes[i:]),\n",
    "                0,\n",
    "                cwes[i:],\n",
    "                max_request_size,\n",
    "                cwe_token_counts[i:],\n",
    "                msg_tc,\n",
    "            )\n",
    "            # print(any(cwe['ID'] == '415' for cwe in cwe_chunk))\n",
    "            i += next_i\n",
    "\n",
    "            # response = '{\"gpt_cwe\":\"415\",\"gpt_cwe_confidence\":5}'\n",
    "            # time.sleep(0.5)\n",
    "\n",
    "            response = gpt_classify(desc, cwe_list_to_json(cwe_chunk))\n",
    "\n",
    "            response = json.loads(response)\n",
    "            # print(response)\n",
    "            responses.append(response)\n",
    "\n",
    "            inner_pbar.update(next_i)\n",
    "\n",
    "        inner_pbar.close()\n",
    "\n",
    "        poss_cwes = filter_possible_cwes(responses)\n",
    "        if len(poss_cwes) == 1:\n",
    "            return poss_cwes[0]\n",
    "        if not poss_cwes: \n",
    "            return {\"gpt_cwe\": '0', 'gpt_cwe_confidence': 5}\n",
    "\n",
    "        cwes = [cwe_dict[cwe[\"gpt_cwe\"]] for cwe in poss_cwes]\n",
    "\n",
    "\n",
    "def classify_issues(cwes, data_df, max_request_size, echo=False):\n",
    "    issue_dict = {\n",
    "        dat.issue_github_id: ([], dat.gpt_description)\n",
    "        for (_, dat) in data_df.iterrows()\n",
    "    }\n",
    "    cwe_dict = {cwe[\"ID\"]: cwe for cwe in cwes}\n",
    "\n",
    "    prompts_dict = load_prompts(os.getcwd() + \"/../utils/prompts\")\n",
    "    msg_tc = get_prompt_preamble_tc(\"\", prompts_dict, True)\n",
    "\n",
    "    i = 0\n",
    "    req_number = 0\n",
    "    chunks = []\n",
    "    while i < len(cwes):\n",
    "        # print(\"i:\", i)\n",
    "        cwe_chunk, next_i, _ = find_cwe_list(\n",
    "            len(cwes[i:]),\n",
    "            0,\n",
    "            cwes[i:],\n",
    "            max_request_size,\n",
    "            cwe_token_counts[i:],\n",
    "            msg_tc,\n",
    "        )\n",
    "        i += next_i\n",
    "\n",
    "        chunks.append(cwe_chunk)\n",
    "        req_number += 1\n",
    "\n",
    "    chunk_iter = tqdm(chunks, desc=\"Processing CWE chunks\", disable=not echo)\n",
    "    for chunk in chunk_iter:\n",
    "        data_iter = tqdm(\n",
    "            data_df.iterrows(),\n",
    "            total=len(data_df),\n",
    "            desc=\"Classifying issues\",\n",
    "            leave=False,\n",
    "            disable=not echo,\n",
    "        )\n",
    "        for _, dat in data_iter:\n",
    "            # response = gpt_classify(dat.gpt_description, cwe_list_to_json(chunk))\n",
    "            # issue_dict[dat.issue_github_id][0].append(response)\n",
    "\n",
    "            time.sleep(0.001)\n",
    "            issue_dict[dat.issue_github_id][0].extend(\n",
    "                [\n",
    "                    {\"gpt_cwe\": \"415\", \"gpt_cwe_confidence\": 5},\n",
    "                    {\"gpt_cwe\": \"664\", \"gpt_cwe_confidence\": 5},\n",
    "                    {\"gpt_cwe\": \"666\", \"gpt_cwe_confidence\": 5},\n",
    "                    {\"gpt_cwe\": \"416\", \"gpt_cwe_confidence\": 5},\n",
    "                    {\"gpt_cwe\": \"1341\", \"gpt_cwe_confidence\": 5},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    data_iter = tqdm(\n",
    "        issue_dict.items(),\n",
    "        desc=\"Re-classifying issues\",\n",
    "        disable=not echo,\n",
    "        leave=False,\n",
    "    )\n",
    "    for key, val in data_iter:\n",
    "        poss_cwes = filter_possible_cwes(val[0])\n",
    "        if len(poss_cwes) == 1:\n",
    "            issue_dict[key] = poss_cwes[0]\n",
    "        else:\n",
    "            remaining_cwes = [cwe_dict[cwe[\"gpt_cwe\"]] for cwe in poss_cwes]\n",
    "            issue_dict[key] = get_gpt_cwe(\n",
    "                remaining_cwes, val[1], max_request_size, echo\n",
    "            )\n",
    "\n",
    "    return issue_dict\n",
    "\n",
    "\n",
    "def batch_cwe_request(\n",
    "    cwes, desc, git_issue_id, max_request_size, model=\"gpt-4o-mini\", echo=True\n",
    "):\n",
    "    prompts_dict = load_prompts(os.getcwd() + \"/../utils/prompts\")\n",
    "    msg_tc = get_prompt_preamble_tc(desc, prompts_dict)\n",
    "\n",
    "    i = 0\n",
    "    req_number = 0\n",
    "    requests = []\n",
    "\n",
    "    inner_pbar = tqdm(\n",
    "        total=len(cwes),\n",
    "        desc=f\"Gathering CWE requests for issue {git_issue_id}\",\n",
    "        leave=False,\n",
    "        disable=not echo,\n",
    "    )\n",
    "    while i < len(cwes):\n",
    "        # print(\"i:\", i)\n",
    "        cwe_chunk, next_i, _ = find_cwe_list(\n",
    "            len(cwes[i:]),\n",
    "            0,\n",
    "            cwes[i:],\n",
    "            max_request_size,\n",
    "            cwe_token_counts[i:],\n",
    "            msg_tc,\n",
    "        )\n",
    "        i += next_i\n",
    "\n",
    "        formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "            cwe_entries=cwe_list_to_json(cwe_chunk)\n",
    "        )\n",
    "        request = {\n",
    "            \"custom_id\": str(git_issue_id) + \"-\" + str(req_number),\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": desc},\n",
    "                ],\n",
    "                \"max_tokens\": 1000,\n",
    "            },\n",
    "        }\n",
    "        requests.append(request)\n",
    "        req_number += 1\n",
    "\n",
    "        inner_pbar.update(next_i)\n",
    "\n",
    "    inner_pbar.close()\n",
    "\n",
    "    return requests\n",
    "\n",
    "\n",
    "def generate_batch_post_files(\n",
    "    cwes,\n",
    "    data_df,\n",
    "    max_request_size,\n",
    "    post_folder_path=\"tmp\",\n",
    "    max_size_mb=150,\n",
    "    output_dir=\"batches\",\n",
    "    output_prefix=\"batch\",\n",
    "    echo=True,\n",
    "):\n",
    "    output_dir = os.path.join(post_folder_path, output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # requests = []\n",
    "    batch_info_dict = {}\n",
    "\n",
    "    max_size_bytes = max_size_mb * 1024 * 1024\n",
    "    part_num = 1\n",
    "    current_size = 0\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"{output_prefix}_{part_num}.jsonl\")\n",
    "    output_file = open(output_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    data_iter = tqdm(\n",
    "        data_df.iterrows(),\n",
    "        total=len(data_df),\n",
    "        desc=\"Writing batch files\",\n",
    "        disable=not echo,\n",
    "    )\n",
    "    for _, row in data_iter:\n",
    "        requests = batch_cwe_request(\n",
    "            cwes, row.gpt_description, row.issue_github_id, max_request_size\n",
    "        )\n",
    "        req_iter = tqdm(\n",
    "            requests,\n",
    "            desc=f\"Writing issue {row.issue_github_id} batch files\",\n",
    "            leave=False,\n",
    "            disable=not echo,\n",
    "        )\n",
    "        for req in req_iter:\n",
    "            req_line = json.dumps(req) + \"\\n\"\n",
    "            line_size = len(req_line.encode(\"utf-8\"))\n",
    "            if current_size + line_size > max_size_bytes:\n",
    "                output_file.close()\n",
    "                part_num += 1\n",
    "                output_path = os.path.join(\n",
    "                    output_dir, f\"{output_prefix}_{part_num}.jsonl\"\n",
    "                )\n",
    "                output_file = open(output_path, \"w\", encoding=\"utf-8\")\n",
    "                current_size = 0\n",
    "            output_file.write(req_line)\n",
    "            current_size += line_size\n",
    "        del requests\n",
    "\n",
    "    output_file.close()\n",
    "    batch_info_dict[\"batch_count\"] = part_num\n",
    "    batch_info_dict[\"batch_prefix\"] = output_prefix\n",
    "    batch_info_dict[\"batch_size\"] = max_size_mb\n",
    "    batch_info_dict[\"batch_issues\"] = data_df[\"issue_github_id\"].tolist()\n",
    "    batch_info_dict[\"batch_dict\"] = {}\n",
    "\n",
    "    pickle_file = os.path.join(output_dir, \"batch_info.pkl\")\n",
    "    with open(pickle_file, \"wb\") as file:\n",
    "        pickle.dump(batch_info_dict, file)\n",
    "\n",
    "\n",
    "def post_batches(post_folder_path=\"tmp\", batches_dir=\"batches\", safe=False):\n",
    "    batches_dir = os.path.join(post_folder_path, batches_dir)\n",
    "    pickle_file = os.path.join(batches_dir, \"batch_info.pkl\")\n",
    "    with open(pickle_file, \"rb\") as file:\n",
    "        batch_info_dict = pickle.load(file)\n",
    "\n",
    "    if safe:\n",
    "        pickle_backup = os.path.join(batches_dir, \"batch_info_backup.pkl\")\n",
    "        with open(pickle_backup, \"wb\") as file:\n",
    "            pickle.dump(batch_info_dict, file)\n",
    "        print(\"Backup of previous batch info successfully saved.\")\n",
    "\n",
    "    # files = []\n",
    "    # batches = {}\n",
    "    for i in tqdm(\n",
    "        range(1, batch_info_dict[\"batch_count\"] + 1),\n",
    "        desc=f\"Posting batches in {batches_dir}\",\n",
    "    ):\n",
    "        batch_name = f\"{batch_info_dict['batch_prefix']}_{i}\"\n",
    "        if (\n",
    "            batch_name in batch_info_dict[\"batch_dict\"]\n",
    "            and batch_info_dict[\"batch_dict\"][batch_name].status == \"completed\"\n",
    "        ):\n",
    "            print(\n",
    "                \"Batch:\",\n",
    "                batch_name,\n",
    "                \"id:\",\n",
    "                batch_info_dict[\"batch_dict\"][batch_name].id,\n",
    "                \"already completed, skipping.\",\n",
    "            )\n",
    "            continue\n",
    "        path = os.path.join(batches_dir, batch_name + \".jsonl\")\n",
    "        file = client.files.create(file=open(path, \"rb\"), purpose=\"batch\")\n",
    "        # files.append(file)\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=file.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"batch_name\": batch_name},\n",
    "        )\n",
    "        batch_info_dict[\"batch_dict\"][batch_name] = batch\n",
    "        print(\"Batch:\", batch_name, \"id:\", batch.id, \"successfully posted.\")\n",
    "\n",
    "    # batch_info_dict[\"batch_ids\"] = list(zip(batches, [\"in_progress\" for _ in range(len(batches))]))\n",
    "    with open(pickle_file, \"wb\") as file:\n",
    "        pickle.dump(batch_info_dict, file)\n",
    "\n",
    "    print(\"All new batch info successfully saved.\")\n",
    "\n",
    "\n",
    "def retrieve_batch_res(\n",
    "    post_folder_path=\"tmp\", batches_dir=\"batches\", output_dir=\"responses\", safe=False\n",
    "):\n",
    "    batches_dir = os.path.join(post_folder_path, batches_dir)\n",
    "    output_dir = os.path.join(batches_dir, output_dir)\n",
    "\n",
    "    pickle_file = os.path.join(batches_dir, \"batch_info.pkl\")\n",
    "    with open(pickle_file, \"rb\") as file:\n",
    "        batch_info_dict = pickle.load(file)\n",
    "\n",
    "    if safe:\n",
    "        pickle_backup = os.path.join(batches_dir, \"batch_info_backup.pkl\")\n",
    "        with open(pickle_backup, \"wb\") as file:\n",
    "            pickle.dump(batch_info_dict, file)\n",
    "        print(\"Backup of previous batch info successfully saved.\")\n",
    "\n",
    "    for batch in tqdm(\n",
    "        batch_info_dict[\"batch_dict\"].values(),\n",
    "        desc=f\"Retrieving results from batches in {batches_dir}\",\n",
    "    ):\n",
    "        status = batch.status\n",
    "        if status == \"completed\":\n",
    "            continue\n",
    "        batch = client.batches.retrieve(batch.id)\n",
    "        status = batch.status\n",
    "        if status == \"completed\":\n",
    "            file_response = client.files.content(batch.output_file_id)\n",
    "            output_file = os.path.join(\n",
    "                output_dir, batch.metadata[\"batch_name\"] + \"_response.jsonl\"\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(file_response.text)\n",
    "            print(\n",
    "                \"batch,\",\n",
    "                batch.metadata[\"batch_name\"],\n",
    "                \"successfully completed, retrieved and saved in\",\n",
    "                output_dir,\n",
    "            )\n",
    "        else:\n",
    "            print(\"batch,\", batch.metadata[\"batch_name\"], \"status:\\t\", status)\n",
    "            # Pretty print errors if available\n",
    "            if hasattr(batch, \"errors\") and hasattr(batch.errors, \"data\"):\n",
    "                print(\"\\nErrors:\")\n",
    "                for error in batch.errors.data:\n",
    "                    print(\n",
    "                        f\"- Line {error.line}: [{error.code}] {error.message} (param: {error.param})\"\n",
    "                    )\n",
    "            else:\n",
    "                print(\"No detailed errors found.\")\n",
    "\n",
    "        batch_info_dict[\"batch_dict\"][batch.metadata[\"batch_name\"]] = batch\n",
    "\n",
    "    with open(pickle_file, \"wb\") as file:\n",
    "        pickle.dump(batch_info_dict, file)\n",
    "\n",
    "    print(\"All new batch info successfully saved.\")\n",
    "\n",
    "    return batch_info_dict\n",
    "\n",
    "\n",
    "def retrieve_req_res(\n",
    "    data_df,\n",
    "    post_folder_path=\"tmp\",\n",
    "    batches_dir=\"batches\",\n",
    "    output_dir=\"responses\",\n",
    "    clean=False,\n",
    "):\n",
    "    batches_dir = os.path.join(post_folder_path, batches_dir)\n",
    "    output_dir = os.path.join(batches_dir, output_dir)\n",
    "\n",
    "    pickle_file = os.path.join(batches_dir, \"batch_info.pkl\")\n",
    "    with open(pickle_file, \"rb\") as file:\n",
    "        batch_info_dict = pickle.load(file)\n",
    "\n",
    "    res_dict = {}\n",
    "    for i in tqdm(\n",
    "        range(1, batch_info_dict[\"batch_count\"] + 1),\n",
    "        desc=f\"Retrieving gpt responses from {output_dir}\",\n",
    "    ):\n",
    "        path = os.path.join(\n",
    "            output_dir, f\"{batch_info_dict['batch_prefix']}_{i}_response.jsonl\"\n",
    "        )\n",
    "        data = []\n",
    "        with open(path, \"r\") as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "\n",
    "        for d in data:\n",
    "            git_id = d[\"custom_id\"].split(\"-\")[0]\n",
    "            resp = d[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            first_brace = resp.find(\"{\")\n",
    "            last_brace = resp.rfind(\"}\")\n",
    "\n",
    "            if first_brace == -1 or last_brace == -1:\n",
    "                raise ValueError(\"No valid JSON found in the string\")\n",
    "\n",
    "            try:\n",
    "                json_resp = json.loads(resp[first_brace : last_brace + 1])\n",
    "            except:\n",
    "                print(\n",
    "                    f\"\"\"Something went wrong when doing `json.loads`, skipping.\\n\n",
    "                    GPT-JSON-response-string:\\n{resp[first_brace : last_brace + 1]}\"\"\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            if clean:\n",
    "                json_resp[\"gpt_cwe_confidence\"] = json_resp.pop(\"gpt_confidence\")\n",
    "\n",
    "            if git_id not in res_dict:\n",
    "                res_dict[git_id] = [json_resp]\n",
    "            else:\n",
    "                res_dict[git_id].append(json_resp)\n",
    "\n",
    "    res_dict = {int(key): filter_possible_cwes(res) for key, res in res_dict.items()}\n",
    "    res_df = pd.DataFrame(\n",
    "        {\"issue_github_id\": res_dict.keys(), \"cwe_candidates\": res_dict.values()}\n",
    "    )\n",
    "    ret_df = pd.merge(data_df, res_df, on=\"issue_github_id\")\n",
    "\n",
    "    return ret_df\n",
    "\n",
    "    # get_gpt_cwe\n",
    "\n",
    "\n",
    "def get_final_res(\n",
    "    data_df, cwes, max_retries=10, delay=1, file_path=\"tmp/gpt_response_few_cwe_df.pkl\"\n",
    "):\n",
    "    cwe_dict = {cwe[\"ID\"]: cwe for cwe in cwes}\n",
    "    ret_df = data_df.copy()\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            ret_df = pickle.load(file)\n",
    "        print(\"Pickle file loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"The file at {file_path} does not exist. Setting gpt_response to 'None'\")\n",
    "        ret_df[\"gpt_response\"] = None\n",
    "    retries = 0\n",
    "    # test_df['gpt_response'] = None\n",
    "    while (not ret_df[ret_df.gpt_response.isna()].empty) and retries < max_retries:\n",
    "        # Get indices of rows needing processing\n",
    "        na_indices = ret_df[\n",
    "            ret_df.gpt_response.isna() & ~ret_df.cwe_candidates.isna()\n",
    "        ].index\n",
    "\n",
    "        if len(na_indices) == 0:\n",
    "            break\n",
    "\n",
    "        # Process ONLY those rows and assign directly to original DF\n",
    "        ret_df.loc[na_indices, \"gpt_response\"] = ret_df.loc[\n",
    "            na_indices, [\"gpt_description\", \"cwe_candidates\"]\n",
    "        ].parallel_apply(\n",
    "            lambda x: get_gpt_cwe(\n",
    "                [\n",
    "                    cwe_dict[cwe[\"gpt_cwe\"]]\n",
    "                    for cwe in x.cwe_candidates\n",
    "                    if cwe[\"gpt_cwe\"] in cwe_dict\n",
    "                ],\n",
    "                x.gpt_description,\n",
    "                100000,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        with open(file_path, \"wb\") as file:  # 'wb' mode writes in binary format\n",
    "            pickle.dump(ret_df, file)\n",
    "        retries += 1\n",
    "        print(f\"Retry {retries}: Processed {len(na_indices)} rows\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return ret_df\n",
    "\n",
    "\n",
    "def cancel_batches(post_folder_path=\"tmp\", batches_dir=\"batches\"):\n",
    "    batches_dir = os.path.join(post_folder_path, batches_dir)\n",
    "\n",
    "    pickle_file = os.path.join(batches_dir, \"batch_info.pkl\")\n",
    "    with open(pickle_file, \"rb\") as file:\n",
    "        batch_info_dict = pickle.load(file)\n",
    "    for name, batch in batch_info_dict[\"batch_dict\"].items():\n",
    "        client.batches.cancel(batch.id)\n",
    "        print(\"Batch:\", name, \"id:\", batch.id, \"canceled.\")\n",
    "\n",
    "\n",
    "def evaluate_baseline(df, col):\n",
    "    y_test = df.cve_primary_cwe\n",
    "    y_pred = df[col]\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # print(len(df[df.cve_primary_cwe == df[col]]) / len(df))\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c67bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(true_pos_few[\"issue_github_id\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f67b9af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa28e80d1fd54b6b804a2a1090be3236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d6f500ee6541979bbd46c6a73f3e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/6.50M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9e7c945b9847769f735b2e28bdcb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_res = load_dataset(\"Eathus/github-issues-vul-label-rag-results\", split=\"test\")\n",
    "rag_res_df = rag_res.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1b52281",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_res_df[\"cwe_candidates\"] = rag_res_df.rag_candidates.map(lambda x: [{'gpt_cwe': str(cwe), 'gpt_cwe_confidence': 5} for cwe in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82b94281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 15 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "The file at tmp/rag_predictions_df.pkl does not exist. Setting gpt_response to 'None'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7e4de1869e41758f24cba6bad6d1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=20), Label(value='0 / 20'))), HBoxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1: Processed 291 rows\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=True, nb_workers=15)\n",
    "\n",
    "final_df = get_final_res(rag_res_df, cwes, file_path=\"tmp/rag_predictions_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c66942d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5051546391752577\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1050       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         1\n",
      "        1176       0.00      0.00      0.00         0\n",
      "         119       0.00      0.00      0.00         7\n",
      "         120       0.44      0.29      0.35        14\n",
      "         121       0.00      0.00      0.00         0\n",
      "         122       0.03      1.00      0.05         1\n",
      "         125       0.75      0.18      0.29        17\n",
      "        1333       0.50      1.00      0.67         1\n",
      "        1335       0.00      0.00      0.00         0\n",
      "        1392       0.00      0.00      0.00         0\n",
      "         150       1.00      1.00      1.00         1\n",
      "         190       0.75      0.75      0.75         4\n",
      "          20       0.25      0.33      0.29         3\n",
      "         200       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         0\n",
      "         266       0.00      0.00      0.00         0\n",
      "         280       0.00      0.00      0.00         1\n",
      "         283       0.00      0.00      0.00         1\n",
      "         295       0.00      0.00      0.00         1\n",
      "         297       0.00      0.00      0.00         0\n",
      "         300       0.00      0.00      0.00         0\n",
      "         306       0.50      1.00      0.67         1\n",
      "          36       0.00      0.00      0.00         0\n",
      "         369       0.50      1.00      0.67         1\n",
      "         372       0.00      0.00      0.00         1\n",
      "          39       0.00      0.00      0.00         0\n",
      "         400       0.00      0.00      0.00         2\n",
      "         401       0.89      0.94      0.92        18\n",
      "         407       0.00      0.00      0.00         1\n",
      "         415       0.75      1.00      0.86         3\n",
      "         416       0.82      0.90      0.86        10\n",
      "         424       0.00      0.00      0.00         0\n",
      "         434       1.00      0.50      0.67         2\n",
      "         476       0.86      0.88      0.87        56\n",
      "         532       1.00      1.00      1.00         3\n",
      "         601       1.00      1.00      1.00         1\n",
      "         617       1.00      0.80      0.89        15\n",
      "         665       0.00      0.00      0.00         1\n",
      "         674       0.00      0.00      0.00         0\n",
      "         681       0.00      0.00      0.00         0\n",
      "         690       0.00      0.00      0.00         0\n",
      "         703       0.00      0.00      0.00         0\n",
      "         704       0.00      0.00      0.00         1\n",
      "         732       0.00      0.00      0.00         2\n",
      "          74       0.00      0.00      0.00         2\n",
      "         754       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "         770       1.00      0.07      0.13        14\n",
      "          78       0.33      1.00      0.50         2\n",
      "         787       0.40      0.07      0.12        56\n",
      "         789       0.00      0.00      0.00         0\n",
      "          79       1.00      1.00      1.00         8\n",
      "         798       0.00      0.00      0.00         2\n",
      "         822       0.00      0.00      0.00         0\n",
      "         824       0.00      0.00      0.00         0\n",
      "         862       0.00      0.00      0.00         1\n",
      "          89       1.00      0.73      0.84        26\n",
      "         913       0.00      0.00      0.00         0\n",
      "         918       1.00      1.00      1.00         1\n",
      "          94       1.00      0.33      0.50         3\n",
      "          95       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.51       291\n",
      "   macro avg       0.27      0.27      0.24       291\n",
      "weighted avg       0.67      0.51      0.53       291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "test_df = final_df.copy()\n",
    "tmp = pd.json_normalize(test_df[\"gpt_response\"])\n",
    "test_df = pd.concat([test_df.drop(columns=[\"gpt_response\"]), tmp], axis=1)\n",
    "evaluate_baseline(test_df, \"gpt_cwe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c5664",
   "metadata": {},
   "source": [
    "## Entire Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6faff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_batch_post_files(cwes, true_pos_few, 100000, max_size_mb=100)\n",
    "# classify_issues(cwes, true_pos_few, 100000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75819282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0273c69c8d4adf89d0f138f6f90e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Posting batches in tmp/batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: batch_1 id: batch_684bf022756c81908e2ee98bfdb30552 successfully posted.\n",
      "Batch: batch_2 id: batch_684bf0c68f3081908fb3d86d0384ac22 successfully posted.\n",
      "Batch: batch_3 id: batch_684bf1599e288190a7dbb5f8b2364976 successfully posted.\n",
      "Batch: batch_4 id: batch_684bf1ea7d108190973fc1b25d029a28 successfully posted.\n",
      "Batch: batch_5 id: batch_684bf1fd5e7081909237ae343bd66515 successfully posted.\n",
      "Batch: batch_6 id: batch_684bf20fc6648190b81d8a73fcd0e6ec successfully posted.\n",
      "Batch: batch_7 id: batch_684bf23e43ec8190acc0d57ffb657e93 successfully posted.\n",
      "Batch: batch_8 id: batch_684bf2500cfc819083f0f531c077b366 successfully posted.\n",
      "Batch: batch_9 id: batch_684bf2600b08819088d2b85e3a8726db successfully posted.\n",
      "Batch: batch_10 id: batch_684bf279085c8190a0a94d58fbbc0aeb successfully posted.\n",
      "Batch: batch_11 id: batch_684bf28b8f748190b9ef9470b21fc985 successfully posted.\n",
      "Batch: batch_12 id: batch_684bf3567cc481909ad31cc17aa00855 successfully posted.\n",
      "Batch: batch_13 id: batch_684bf3f1d69c8190bcaf29c0ef217652 successfully posted.\n",
      "Batch: batch_14 id: batch_684bf4a4da088190816f012cb6203c69 successfully posted.\n",
      "Batch: batch_15 id: batch_684bf4d23ea48190b3e7fc6a3e6a2b03 successfully posted.\n",
      "Batch: batch_16 id: batch_684bf55857288190b852e45a9aa60a54 successfully posted.\n",
      "Batch: batch_17 id: batch_684bf568bfb881908f65a69201804dc2 successfully posted.\n",
      "Batch: batch_18 id: batch_684bf578969c819089cd981310fae911 successfully posted.\n",
      "Batch: batch_19 id: batch_684bf61255e08190945c1f53c23bf8cb successfully posted.\n",
      "Batch: batch_20 id: batch_684bf6c39a0881908fbabce7995fc1b3 successfully posted.\n",
      "Batch: batch_21 id: batch_684bf746e0e48190a004ee2b84a27649 successfully posted.\n",
      "Batch: batch_22 id: batch_684bf7e379a48190836c95e5064dc6c7 successfully posted.\n",
      "Batch: batch_23 id: batch_684bf868ff388190a66d5ab7d792d004 successfully posted.\n",
      "Batch: batch_24 id: batch_684bf8f01fa48190a1a06f285851c32a successfully posted.\n",
      "Batch: batch_25 id: batch_684bf995a24c8190bbccbf749deb6b81 successfully posted.\n",
      "Batch: batch_26 id: batch_684bfa432e648190a7c7fca81548edd9 successfully posted.\n",
      "Batch: batch_27 id: batch_684bfae4fc8c81908551fecb64395194 successfully posted.\n",
      "Batch: batch_28 id: batch_684bfaf719b08190aed8ed821db08944 successfully posted.\n",
      "Batch: batch_29 id: batch_684bfb0aa9d48190987c1bc5d614be6b successfully posted.\n",
      "Batch: batch_30 id: batch_684bfb1d6e2481909c7f39a41eed4329 successfully posted.\n",
      "Batch: batch_31 id: batch_684bfb2f3c208190b0b18f8453e540ed successfully posted.\n",
      "Batch: batch_32 id: batch_684bfb3f49c0819087fc580afe161407 successfully posted.\n",
      "Batch: batch_33 id: batch_684bfb50f2288190955e2fd4ffbf3ef0 successfully posted.\n",
      "Batch: batch_34 id: batch_684bfb60e1688190a7c6c4522e51e3fe successfully posted.\n",
      "Batch: batch_35 id: batch_684bfb7294988190bd92d5cb2e8a18e8 successfully posted.\n",
      "Batch: batch_36 id: batch_684bfb82e294819099e5dfe1d0c9446a successfully posted.\n",
      "Batch: batch_37 id: batch_684bfc3a2380819093d85a637c752dc0 successfully posted.\n",
      "Batch: batch_38 id: batch_684bfceacf8c8190afd656c6decf4931 successfully posted.\n",
      "Batch: batch_39 id: batch_684bfcfb6fec8190b303d05756d63525 successfully posted.\n",
      "Batch: batch_40 id: batch_684bfd00eadc8190a4d90f16c7db4513 successfully posted.\n",
      "All new batch info successfully saved.\n"
     ]
    }
   ],
   "source": [
    "post_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cca99ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b64dc4311a843759c2f6279f1d5bc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving results from batches in tmp/batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch, batch_37 successfully completed, retrieved and saved in tmp/batches/responses\n",
      "All new batch info successfully saved.\n"
     ]
    }
   ],
   "source": [
    "batch_info = retrieve_batch_res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb356482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a17751d153844cf9b4818fc7073bcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving gpt responses from tmp/batches/responses:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong when doing `json.loads`, skipping.\n",
      "\n",
      "                    GPT-JSON-response-string:\n",
      "{\n",
      "    \"gpt_cwe\": \"';\n",
      "    \"gpt_cwe_confidence\": 5\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "df = retrieve_req_res(true_pos_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fea4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cve_id', 'cve_published', 'cve_descriptions', 'cve_metrics',\n",
       "       'cve_references', 'cve_configurations', 'cve_primary_cwe', 'cve_tags',\n",
       "       'issue_owner_repo', 'issue_body', 'issue_title', 'issue_comments_url',\n",
       "       'issue_comments_count', 'issue_created_at', 'issue_updated_at',\n",
       "       'issue_html_url', 'issue_github_id', 'issue_number', 'label',\n",
       "       'issue_msg', 'issue_msg_n_tokens', 'issue_embedding',\n",
       "       '__index_level_0__', 'gpt_description', 'gpt_vulnerability',\n",
       "       'gpt_confidence', 'gpt_is_relevant', 'cwe_candidates'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cwe_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "548b2bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 15 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "Pickle file loaded successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa9638baa8047598c1149b34f497765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1), Label(value='0 / 1'))),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1: Processed 1 rows\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=True, nb_workers=15)\n",
    "\n",
    "final_df = get_final_res(df, cwes, file_path=\"tmp/baseline_predictions_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f11b6b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'415'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.gpt_response.iloc[0]['gpt_cwe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0bf9416e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'415'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.gpt_cwe.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4ec22b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4639175257731959\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "        1050       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         1\n",
      "         117       0.00      0.00      0.00         0\n",
      "         119       0.00      0.00      0.00         7\n",
      "         120       0.60      0.43      0.50        14\n",
      "         121       0.00      0.00      0.00         0\n",
      "         122       0.02      1.00      0.05         1\n",
      "         125       0.67      0.12      0.20        17\n",
      "        1284       0.00      0.00      0.00         0\n",
      "         129       0.00      0.00      0.00         0\n",
      "        1333       1.00      1.00      1.00         1\n",
      "        1392       0.00      0.00      0.00         0\n",
      "         150       0.00      0.00      0.00         1\n",
      "         190       0.60      0.75      0.67         4\n",
      "          20       0.14      0.33      0.20         3\n",
      "         200       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "         221       0.00      0.00      0.00         0\n",
      "          23       0.00      0.00      0.00         0\n",
      "         248       0.00      0.00      0.00         0\n",
      "         280       0.00      0.00      0.00         1\n",
      "         283       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         0\n",
      "         285       0.00      0.00      0.00         0\n",
      "         288       0.00      0.00      0.00         0\n",
      "         295       0.00      0.00      0.00         1\n",
      "         297       0.00      0.00      0.00         0\n",
      "         306       0.00      0.00      0.00         1\n",
      "         369       0.50      1.00      0.67         1\n",
      "         372       0.00      0.00      0.00         1\n",
      "         390       0.00      0.00      0.00         0\n",
      "         400       0.00      0.00      0.00         2\n",
      "         401       0.93      0.78      0.85        18\n",
      "         404       0.00      0.00      0.00         0\n",
      "         407       0.00      0.00      0.00         1\n",
      "         415       0.60      1.00      0.75         3\n",
      "         416       0.82      0.90      0.86        10\n",
      "         434       1.00      0.50      0.67         2\n",
      "         457       0.00      0.00      0.00         0\n",
      "         476       0.88      0.93      0.90        56\n",
      "         532       1.00      0.67      0.80         3\n",
      "         601       1.00      1.00      1.00         1\n",
      "         617       1.00      0.53      0.70        15\n",
      "         665       0.00      0.00      0.00         1\n",
      "         674       0.00      0.00      0.00         0\n",
      "         681       0.00      0.00      0.00         0\n",
      "         684       0.00      0.00      0.00         0\n",
      "         703       0.00      0.00      0.00         0\n",
      "         704       0.00      0.00      0.00         1\n",
      "         732       0.00      0.00      0.00         2\n",
      "          74       0.00      0.00      0.00         2\n",
      "         754       0.00      0.00      0.00         1\n",
      "         755       0.00      0.00      0.00         0\n",
      "         763       0.00      0.00      0.00         0\n",
      "          77       0.00      0.00      0.00         1\n",
      "         770       0.00      0.00      0.00        14\n",
      "          78       0.33      1.00      0.50         2\n",
      "         787       0.67      0.04      0.07        56\n",
      "         789       0.00      0.00      0.00         0\n",
      "          79       1.00      1.00      1.00         8\n",
      "         798       0.00      0.00      0.00         2\n",
      "         822       0.00      0.00      0.00         0\n",
      "         824       0.00      0.00      0.00         0\n",
      "         862       0.00      0.00      0.00         1\n",
      "         863       0.00      0.00      0.00         0\n",
      "          89       1.00      0.62      0.76        26\n",
      "         908       0.00      0.00      0.00         0\n",
      "         911       0.00      0.00      0.00         0\n",
      "         918       1.00      1.00      1.00         1\n",
      "         923       0.00      0.00      0.00         0\n",
      "          94       0.00      0.00      0.00         3\n",
      "          95       0.00      0.00      0.00         0\n",
      "        None       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.46       291\n",
      "   macro avg       0.21      0.21      0.19       291\n",
      "weighted avg       0.67      0.46      0.49       291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "test_df = final_df.copy()\n",
    "tmp = pd.json_normalize(test_df[\"gpt_response\"])\n",
    "test_df = pd.concat([test_df.drop(columns=[\"gpt_response\"]), tmp], axis=1)\n",
    "evaluate_baseline(test_df, \"gpt_cwe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c318ad",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e837ea5",
   "metadata": {},
   "source": [
    "### Normal Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_batch_post_files(\n",
    "    cwes, true_pos_few[0:30], 100000, max_size_mb=100, output_dir=\"test_batches\"\n",
    ")\n",
    "# classify_issues(cwes, true_pos_few, 100000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74775542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: batch_1 id: batch_684b4f0014248190952540fdd22ae275 successfully posted.\n",
      "Batch: batch_2 id: batch_684b4f11a3e08190b2a0ed470e2f2c77 successfully posted.\n",
      "Batch: batch_3 id: batch_684b4f22adfc819088af7b850ec626f7 successfully posted.\n",
      "Batch: batch_4 id: batch_684b4f31a2cc8190b4e7b42ca821e4ea successfully posted.\n",
      "Batch: batch_5 id: batch_684b4f339d94819092704b543873dc63 successfully posted.\n",
      "All batch info successfully saved.\n"
     ]
    }
   ],
   "source": [
    "post_batches(batches_dir=\"test_batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "501b6540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0ab29b64c84ace8faf1e4c65083520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving results from batches in tmp/test_batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All new batch info successfully saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_info = retrieve_batch_res(batches_dir=\"test_batches\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9998bba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f8bb2bff4245999a908f7ecba40bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving gpt responses from tmp/test_batches/responses:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = retrieve_req_res(true_pos_few[0:30], batches_dir=\"test_batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4a6736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 15 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "The file at tmp/baseline_test_predictions_df.pkl does not exist. Setting gpt_response to 'None'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ced7a371664670bcb66334263c0afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2), Label(value='0 / 2'))), HBox(câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1: Processed 30 rows\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=True, nb_workers=15)\n",
    "\n",
    "final_df = get_final_res(df, cwes, file_path=\"tmp/baseline_test_predictions_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f0d812e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       {'gpt_cwe': '415', 'gpt_cwe_confidence': 5}\n",
       "1      {'gpt_cwe': '1392', 'gpt_cwe_confidence': 5}\n",
       "2       {'gpt_cwe': '122', 'gpt_cwe_confidence': 5}\n",
       "3        {'gpt_cwe': '89', 'gpt_cwe_confidence': 5}\n",
       "4        {'gpt_cwe': '95', 'gpt_cwe_confidence': 5}\n",
       "                           ...                     \n",
       "286     {'gpt_cwe': '122', 'gpt_cwe_confidence': 5}\n",
       "287     {'gpt_cwe': '404', 'gpt_cwe_confidence': 4}\n",
       "288      {'gpt_cwe': '20', 'gpt_cwe_confidence': 5}\n",
       "289     {'gpt_cwe': '863', 'gpt_cwe_confidence': 5}\n",
       "290      {'gpt_cwe': '79', 'gpt_cwe_confidence': 5}\n",
       "Name: gpt_response, Length: 291, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.gpt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9dcda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = final_df.copy()\n",
    "tmp = pd.json_normalize(test_df[\"gpt_response\"])\n",
    "test_df = pd.concat([test_df.drop(columns=[\"gpt_response\"]), tmp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa39b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.43333333333333335\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         119       0.00      0.00      0.00         1\n",
      "         120       0.00      0.00      0.00         0\n",
      "         121       0.00      0.00      0.00         0\n",
      "         122       0.00      0.00      0.00         0\n",
      "         125       0.00      0.00      0.00         1\n",
      "        1392       0.00      0.00      0.00         0\n",
      "         190       1.00      1.00      1.00         2\n",
      "          20       0.00      0.00      0.00         0\n",
      "         200       0.00      0.00      0.00         0\n",
      "         400       0.00      0.00      0.00         1\n",
      "         401       1.00      1.00      1.00         2\n",
      "         415       1.00      1.00      1.00         1\n",
      "         416       1.00      0.50      0.67         2\n",
      "         476       0.67      1.00      0.80         4\n",
      "         532       1.00      1.00      1.00         1\n",
      "         617       0.00      0.00      0.00         2\n",
      "          74       0.00      0.00      0.00         2\n",
      "         754       0.00      0.00      0.00         1\n",
      "         787       0.00      0.00      0.00         6\n",
      "         798       0.00      0.00      0.00         1\n",
      "         862       0.00      0.00      0.00         1\n",
      "          89       1.00      1.00      1.00         2\n",
      "          94       0.00      0.00      0.00         0\n",
      "          95       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.43        30\n",
      "   macro avg       0.28      0.27      0.27        30\n",
      "weighted avg       0.42      0.43      0.42        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate_baseline(test_df, \"gpt_cwe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cf98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_dir = os.path.join(\"tmp\", \"test_batches\")\n",
    "pickle_file = os.path.join(batches_dir, \"batch_info.pkl\")\n",
    "with open(pickle_file, \"rb\") as file:\n",
    "    batch_info_dict = pickle.load(file)\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54193f4",
   "metadata": {},
   "source": [
    "### Small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_batch_post_files(\n",
    "    cwes, true_pos_few[0:3], 100000, max_size_mb=190, output_dir=\"test_batches_small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2aff21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_batches(batches_dir=\"test_batches_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d093737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch, batch_1 successfully completed, retrieved and saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_count': 1,\n",
       " 'batch_prefix': 'batch',\n",
       " 'batch_size': 190,\n",
       " 'batch_issues': [670848928, 2581924154, 1110212655],\n",
       " 'batch_dict': {'batch_1': Batch(id='batch_6847eadb3f888190a12fdebbbd98681a', completion_window='24h', created_at=1749543643, endpoint='/v1/chat/completions', input_file_id='file-1Qx3fGTEijL7sVs3MMrwYS', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1749596983, error_file_id=None, errors=None, expired_at=None, expires_at=1749630043, failed_at=None, finalizing_at=1749596973, in_progress_at=1749543646, metadata={'batch_name': 'batch_1'}, output_file_id='file-3jVrRWHAGM66fk1ZbEmBHr', request_counts=BatchRequestCounts(completed=75, failed=0, total=75))}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_batch_res(batches_dir=\"test_batches_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21d6cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = retrieve_req_res(true_pos_few[0:3], batches_dir=\"test_batches_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a2e05df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'gpt_cwe': '399', 'gpt_cwe_confidence': 5}, ...\n",
       "1    [{'gpt_cwe': '284', 'gpt_cwe_confidence': 5}, ...\n",
       "2    [{'gpt_cwe': '122', 'gpt_cwe_confidence': 5}, ...\n",
       "3    [{'gpt_cwe': '89', 'gpt_cwe_confidence': 5}, {...\n",
       "4    [{'gpt_cwe': '787', 'gpt_cwe_confidence': 5}, ...\n",
       "Name: cwe_candidates, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.cwe_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c50f641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 15 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "The file at tmp/baseline_small_test_predictions_df.pkl does not exist. Setting gpt_response to 'None'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8533119f53b4d6d9834a304a239519d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1), Label(value='0 / 1'))), HBox(câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1: Processed 3 rows\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=True, nb_workers=15)\n",
    "\n",
    "final_df = get_final_res(\n",
    "    df, cwes, file_path=\"tmp/baseline_small_test_predictions_df.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52220b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = final_df.copy()\n",
    "tmp = pd.json_normalize(test_df[\"gpt_response\"])\n",
    "test_df = pd.concat([test_df.drop(columns=[\"gpt_response\"]), tmp], axis=1)\n",
    "# test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37a585e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         122       0.00      0.00      0.00         0\n",
      "        1392       0.00      0.00      0.00         0\n",
      "         415       1.00      1.00      1.00         1\n",
      "         416       0.00      0.00      0.00         1\n",
      "         798       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.20      0.20      0.20         3\n",
      "weighted avg       0.33      0.33      0.33         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/edvinn/anaconda3/envs/mthesis_cpyenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate_baseline(test_df, \"gpt_cwe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076ec3e",
   "metadata": {},
   "source": [
    "### Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff02f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50511276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_6846d63671f481909f1dfe6860ba11e7', completion_window='24h', created_at=1749472822, endpoint='/v1/chat/completions', input_file_id='file-Jgh11BRXqDzafZfXvsoDUt', object='batch', status='cancelling', cancelled_at=None, cancelling_at=1749499802, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1749559222, failed_at=None, finalizing_at=None, in_progress_at=1749472825, metadata={'batch_name': 'batch_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=125))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.batches.cancel(\"batch_6846d63671f481909f1dfe6860ba11e7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "175e1d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'John Doe',\n",
       " 'age': 25,\n",
       " 'is_student': False,\n",
       " 'courses': ['Mathematics', 'Computer Science', 'Physics'],\n",
       " 'address': {'street': '123 Main St',\n",
       "  'city': 'Stockholm',\n",
       "  'postal_code': '12345'}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string = \"\"\"\n",
    "{\\n\n",
    "    \"name\": \"John Doe\",\n",
    "    \"age\": 25,\n",
    "    \"is_student\": false,\n",
    "    \"courses\": [\"Mathematics\", \"Computer Science\", \"Physics\"],\n",
    "    \"address\": {\n",
    "        \"street\": \"123 Main St\",\n",
    "        \"city\": \"Stockholm\",\n",
    "        \"postal_code\": \"12345\"\n",
    "    }\\n\n",
    "}\n",
    "\"\"\"\n",
    "json.loads(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0147fe0f",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77692db",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"custom_id\": \"request-1\",\n",
    "    \"method\": \"POST\",\n",
    "    \"url\": \"/v1/chat/completions\",\n",
    "    \"body\": {\n",
    "        \"model\": \"gpt-3.5-turbo-0125\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello world!\"},\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "    },\n",
    "}\n",
    "{\n",
    "    \"custom_id\": \"request-2\",\n",
    "    \"method\": \"POST\",\n",
    "    \"url\": \"/v1/chat/completions\",\n",
    "    \"body\": {\n",
    "        \"model\": \"gpt-3.5-turbo-0125\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello world!\"},\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(cwe_entries=\"\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"\"},\n",
    "]\n",
    "msg_tc = count_chat_tokens(messages)\n",
    "cwe_chunk, end_ind, size = find_cwe_list(\n",
    "    len(cwes[186:]), 0, cwes[186:], 100000, cwe_token_counts[186:], msg_tc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "    cwe_entries=cwe_list_to_json(cwe_chunk)\n",
    ")\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": formatted_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": true_pos_few.iloc[0].gpt_description},\n",
    "]\n",
    "print(\"end_ind:\\t\", end_ind)\n",
    "print(\"true size:\\t\", count_chat_tokens(msgs))\n",
    "print(\"size:\\t\", size)\n",
    "print(\"len(cwe_chunk):\\t\", len(cwe_chunk))\n",
    "print(\"cwe_chunk:\\t\", cwe_list_to_json(cwe_chunk))\n",
    "# display(cwe_list_to_json(cwe_chunk, 4))\n",
    "\n",
    "formatted_system_prompt = prompts_dict[\"baseline_system_setup\"].format(\n",
    "    cwe_entries=cwe_list_to_json(cwe_chunk)\n",
    ")\n",
    "display(formatted_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26941647",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cwes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e731b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxr_size = 100000\n",
    "resp = get_gpt_cwe(cwes, true_pos_few.iloc[0].gpt_description, maxr_size, echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(resp))\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mthesis_cpyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
