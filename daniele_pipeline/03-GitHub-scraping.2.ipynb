{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a5e236d2adbc17",
   "metadata": {},
   "source": [
    "## Repository selection\n",
    "We want to list and select repositories high number of CVEs references to them.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4b8534a8bccdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:14:32.112095Z",
     "start_time": "2024-08-21T14:14:25.140890Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from  datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('Eathus/cve-references-list', split='train')\n",
    "references_df = ds.to_pandas()\n",
    "\n",
    "ds = load_dataset('Eathus/filtered-vulnerabilities', split='train')\n",
    "vulnerabilities_df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364a1a87df1cc03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:14:32.175893Z",
     "start_time": "2024-08-21T14:14:32.113486Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_df = references_df[references_df['url'].str.contains('github.*issues')]\n",
    "#ref_df = ref_df[pd.to_datetime(ref_df.published) > pd.to_datetime('2023-05-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db206c5111822a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T14:14:32.183961Z",
     "start_time": "2024-08-21T14:14:32.176781Z"
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "from tqdm import tqdm\n",
    "\n",
    "repos = set()\n",
    "\n",
    "for id, url in tqdm(ref_df[['id','url']].itertuples(index=False)):\n",
    "    \n",
    "    pattern = r'https://github.com/([^/]+)/([^/]+)/issues/'\n",
    "    match = re.match(pattern, url)\n",
    "    if match:\n",
    "        owner = match.group(1)\n",
    "        repo = match.group(2)\n",
    "        repos.add((owner, repo, id))\n",
    "        \n",
    "print(len(repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd66769d8586eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:20:21.710633Z",
     "start_time": "2024-08-12T17:20:21.675410Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(repos, columns=['owner', 'repo_name', 'cve'])\n",
    "df_group = df.groupby(['owner', 'repo_name']).count()\n",
    "df_group_sorted = df_group.sort_values(by='cve', ascending=False)\n",
    "print(sum(df_group_sorted.cve.tolist()))\n",
    "top_50_groups = df_group_sorted.head(50)\n",
    "top_50_groups.head(50)\n",
    "print(sum(top_50_groups.cve.tolist()))\n",
    "display(df_group_sorted.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2fc00d41ee562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:20:23.703119Z",
     "start_time": "2024-08-12T17:20:23.700106Z"
    }
   },
   "outputs": [],
   "source": [
    "repos_set = set(df_group_sorted.index)\n",
    "display(repos_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f88ae21f23a697",
   "metadata": {},
   "source": [
    "## Select all the issues/pull form  CVEs references that are linked to one of those repos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308572b60bb83d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:20:39.470558Z",
     "start_time": "2024-08-12T17:20:39.345628Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_references_df_tmp = references_df[\n",
    "    references_df['url'].str.contains('github.com/[^/]+/[^/]+/issues/[0-9]+', regex=True)].copy()\n",
    "\n",
    "\n",
    "def funct(url):\n",
    "    pattern = r'https://github.com/([^/]+)/([^/]+)/'\n",
    "    match = re.match(pattern, url)\n",
    "    if match:\n",
    "        owner = match.group(1)\n",
    "        repo = match.group(2)\n",
    "        return (owner, repo)\n",
    "    return None\n",
    "\n",
    "\n",
    "filtered_references_df_tmp['owner_repo'] = filtered_references_df_tmp['url'].map(funct)\n",
    "filtered_references_df = filtered_references_df_tmp[filtered_references_df_tmp.owner_repo.isin(repos_set)]\n",
    "filtered_references_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777a71394274ab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:20:41.020674Z",
     "start_time": "2024-08-12T17:20:41.009730Z"
    }
   },
   "outputs": [],
   "source": [
    "print('filtered_references_df len:\\t', len(filtered_references_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761008705eb247b0",
   "metadata": {},
   "source": [
    "## Scrape issues from GitHub referenced Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44060ea71ef56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:20:54.865825Z",
     "start_time": "2024-08-12T17:20:54.857664Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "USER_MAIN = os.getenv(\"GITHUB_USER_MAIN\")\n",
    "USER_PRIV = os.getenv(\"GITHUB_USER_PRIV\")\n",
    "USER_SCHOOL = os.getenv(\"GITHUB_USER_SCHOOL\")\n",
    "\n",
    "API_KEY_MAIN = os.getenv(\"GITHUB_API_KEY_MAIN\")\n",
    "API_KEY_PRIV = os.getenv(\"GITHUB_API_KEY_PRIV\")\n",
    "API_KEY_SCHOOL = os.getenv(\"GITHUB_API_KEY_SCHOOL\")\n",
    "counter = 0\n",
    "dict_auth = [\n",
    "    {\n",
    "        'user': USER_MAIN,\n",
    "        'secret': API_KEY_MAIN\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'user': USER_PRIV,\n",
    "        'secret': API_KEY_PRIV\n",
    "    },\n",
    "    {\n",
    "        'user': USER_SCHOOL,\n",
    "        'secret': API_KEY_SCHOOL\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def get_issue(url_html):\n",
    "    \n",
    "    url = url_html.replace(\"github.com\", \"api.github.com/repos\")\n",
    "    return _getter(url, url_html)\n",
    "\n",
    "\n",
    "def _getter(url, html_url):\n",
    "    global counter\n",
    "\n",
    "    counter += 1\n",
    "    auth = (dict_auth[counter % 3]['user'], dict_auth[counter % 3]['secret'])\n",
    "    response = requests.get(url, auth=auth)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error for URL: {url}\")\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        print(f\"Response: {response.json()}\")\n",
    "        return  # Skip this URL\n",
    "    \n",
    "    res = response.json()\n",
    "    try:\n",
    "        yield {\n",
    "            \"url\": html_url,\n",
    "            \"body\": res['body'],\n",
    "            \"title\": res['title'],\n",
    "            \"comments_url\": res['comments_url'],\n",
    "            \"comments_count\": res['comments'],\n",
    "            \"created_at\": res['created_at'],\n",
    "            \"updated_at\": res[\"updated_at\"],\n",
    "            \"html_url\": res[\"html_url\"],\n",
    "            \"github_id\": res[\"id\"],\n",
    "            \"number\": res[\"number\"]\n",
    "    \n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(f\"Exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a529c63dfa574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:24:58.309213Z",
     "start_time": "2024-08-12T17:20:55.505648Z"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for index, row in tqdm(filtered_references_df.iterrows()):\n",
    "    for item in get_issue(row['url']):\n",
    "        data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce757151",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(data).head())\n",
    "print(len(pd.DataFrame(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5efefaf0cd3ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:07:56.059235Z",
     "start_time": "2024-08-12T17:07:56.015721Z"
    }
   },
   "outputs": [],
   "source": [
    "issues_reference_df = pd.merge(\n",
    "    filtered_references_df,\n",
    "    pd.DataFrame(data),\n",
    "    on='url'\n",
    ")\n",
    "print(len(issues_reference_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a84223",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_reference_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49b86546846384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:07:56.717512Z",
     "start_time": "2024-08-12T17:07:56.665079Z"
    }
   },
   "outputs": [],
   "source": [
    "non_filtered_count = len(issues_reference_df)\n",
    "total_df = pd.merge(vulnerabilities_df, issues_reference_df, on='id')\n",
    "total_df = total_df.drop('published_y', axis=1)\n",
    "display(total_df.head(5))\n",
    "non_na_count = len(total_df.dropna(subset='primary_cwe'))\n",
    "print('non filtered issue count\\t', non_filtered_count)\n",
    "print('filtered issue count:\\t', len(total_df))\n",
    "print('non na issue count:\\t', non_na_count)\n",
    "print('percentage of filtered:\\t', non_na_count / len(total_df))\n",
    "print('percentage of non filtered:\\t', non_na_count / non_filtered_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0762662077a523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:07:57.129124Z",
     "start_time": "2024-08-12T17:07:57.122Z"
    }
   },
   "outputs": [],
   "source": [
    "total_df = total_df.drop(columns=['__index_level_0__'])\n",
    "total_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e6b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.drop(columns=['weaknesses', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763da199dddc20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:08:03.048447Z",
     "start_time": "2024-08-12T17:08:03.025553Z"
    }
   },
   "outputs": [],
   "source": [
    "d = {\n",
    "    'id': 'cve_id',\n",
    "    'published_x': 'cve_published',\n",
    "    'descriptions': 'cve_descriptions',\n",
    "    'metrics': 'cve_metrics',\n",
    "    'references': 'cve_references',\n",
    "    'configurations': 'cve_configurations',\n",
    "    'cwe_list': 'cve_cwe_list',\n",
    "    'primary_cwe': 'cve_primary_cwe',\n",
    "    'tags': 'cve_tags',\n",
    "    'owner_repo': 'issue_owner_repo',\n",
    "    'body': 'issue_body',\n",
    "    'title': 'issue_title',\n",
    "    'comments_url': 'issue_comments_url',\n",
    "    'comments_count': 'issue_comments_count',\n",
    "    'created_at': 'issue_created_at',\n",
    "    'updated_at': 'issue_updated_at',\n",
    "    'html_url': 'issue_html_url',\n",
    "    'github_id': 'issue_github_id',\n",
    "    'number': 'issue_number'\n",
    "}\n",
    "total_df =total_df.rename(columns=d)\n",
    "display(total_df.sample(1))\n",
    "len(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.dropna(subset='cve_primary_cwe')\n",
    "display(total_df.columns)\n",
    "len(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae40da6533ea2da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:08:30.221902Z",
     "start_time": "2024-08-12T17:08:25.566652Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.Dataset.from_pandas(total_df)\n",
    "#dataset = ds.remove_columns(['__index_level_0__'])\n",
    "dataset.push_to_hub(\"Eathus/github-issues-references-max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d05bc4f3d006d",
   "metadata": {},
   "source": [
    "## All issues of top 50 repos (negative + positive dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25989753ef31539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T17:08:30.237081Z",
     "start_time": "2024-08-12T17:08:30.227813Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "counter = 0\n",
    "USER_MAIN = os.getenv(\"GITHUB_USER_MAIN\")\n",
    "USER_PRIV = os.getenv(\"GITHUB_USER_PRIV\")\n",
    "USER_SCHOOL = os.getenv(\"GITHUB_USER_SCHOOL\")\n",
    "\n",
    "API_KEY_MAIN = os.getenv(\"GITHUB_API_KEY_MAIN\")\n",
    "API_KEY_PRIV = os.getenv(\"GITHUB_API_KEY_PRIV\")\n",
    "API_KEY_SCHOOL = os.getenv(\"GITHUB_API_KEY_SCHOOL\")\n",
    "counter = 0\n",
    "dict_auth = [\n",
    "    {\n",
    "        'user': USER_MAIN,\n",
    "        'secret': API_KEY_MAIN\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'user': USER_PRIV,\n",
    "        'secret': API_KEY_PRIV\n",
    "    },\n",
    "    {\n",
    "        'user': USER_SCHOOL,\n",
    "        'secret': API_KEY_SCHOOL\n",
    "    }\n",
    "]\n",
    "    \n",
    "def get_issues(username, repo, ipp=100):\n",
    "    \n",
    "    \n",
    "    tmpl = f\"https://api.github.com/repos/{username}/{repo}/issues?state=all&page=1&per_page={ipp}\"\n",
    "    url = tmpl.format(username=username, repo=repo)\n",
    "    return _getter(url)\n",
    "\n",
    "\n",
    "def _getter(url):\n",
    "    global  counter\n",
    "\n",
    "    counter += 1\n",
    "    link = dict(next=url)\n",
    "    while 'next' in link:\n",
    "        auth = (dict_auth[counter%3]['user'], dict_auth[counter%3]['secret'])\n",
    "        response = requests.get(link['next'], auth=auth, timeout=100)\n",
    "    \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error for URL: {url}\")\n",
    "            print(f\"Status Code: {response.status_code}\")\n",
    "            print(f\"Response: {response.json()}\")\n",
    "            return  # Skip this URL\n",
    "    \n",
    "        '''\n",
    "        if response.status_code != 200:\n",
    "            raise IOError(\n",
    "                \"Non-200 status code %r; %r; %r\" % (\n",
    "                    response.status_code, url, response.json()))\n",
    "        '''\n",
    "        for result in response.json():\n",
    "            yield result\n",
    "    \n",
    "        link = _link_field_to_dict(response.headers.get('link', None))\n",
    "\n",
    "def _link_field_to_dict(field):\n",
    "\n",
    "    if not field:\n",
    "        return dict()\n",
    "\n",
    "    return dict([\n",
    "        (\n",
    "            part.split('; ')[1][5:-1],\n",
    "            part.split('; ')[0][1:-1],\n",
    "        ) for part in field.split(', ')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761a846eb68bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "repos_lists = []\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "file_path = \"tmp/neg_issues.pkl\"\n",
    "save_freq = 10\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File size: {file_size} bytes\")\n",
    "    \n",
    "    if file_size == 0:\n",
    "        print(\"Error: File is empty.\")\n",
    "    else:\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                issues = pickle.load(file)\n",
    "            print(\"Data loaded successfully.\")\n",
    "        except (pickle.PickleError, EOFError) as e:\n",
    "            print(f\"Error loading the pickle file (corrupted or incomplete): {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"The file '{file_path}' does not exist. issues = empty dict\")\n",
    "    issues = {}\n",
    "\n",
    "iter_set = repos_set.difference(issues.keys())\n",
    "for i, (owner, repo) in enumerate(tqdm(iter_set, desc=\"Processing repos\", position=0)):   \n",
    "    if (owner, repo) in issues : \n",
    "        continue\n",
    "    repo_issues = []\n",
    "    for issue in tqdm(\n",
    "        get_issues(owner, repo, 100),\n",
    "        desc=f\"Fetching {owner}/{repo} issues\",\n",
    "        position=1,\n",
    "        leave=False  # Clears the inner bar when done\n",
    "    ):\n",
    "        if \"pull_request\" not in issue.keys():\n",
    "            item = {\n",
    "                \"issue_owner_repo\": (owner, repo),\n",
    "                \"issue_body\": issue['body'],\n",
    "                \"issue_title\":issue['title'],\n",
    "                \"issue_comments_url\": issue['comments_url'],\n",
    "                \"issue_comments_count\": issue['comments'],\n",
    "                \"issue_created_at\": issue['created_at'],\n",
    "                \"issue_updated_at\": issue[\"updated_at\"],\n",
    "                \"issue_html_url\": issue[\"html_url\"],\n",
    "                \"issue_github_id\": issue[\"id\"],\n",
    "                \"issue_number\": issue[\"number\"],\n",
    "            }\n",
    "            repo_issues.append(item)\n",
    "    issues[(owner, repo)] = repo_issues\n",
    "    if (i + 1) % save_freq == 0  or  i == len(iter_set) - 1:\n",
    "        with open(file_path, \"wb\") as file:  # 'wb' mode writes in binary format\n",
    "            pickle.dump(issues, file,  protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            file.flush()  # Force write to disk\n",
    "            os.fsync(file.fileno()) \n",
    "        \n",
    "        tqdm.write(f\"ðŸ”„ Auto-saved at {i + 1} repos\", end=\"\\r\")\n",
    "                \n",
    "print(\"\\nâœ… Final save completed\")\n",
    "                \n",
    "df = pd.DataFrame(sum(issues.values(), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(issues.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d9b7e8094e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sum(issues.values(), []))\n",
    "display(len(df))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5487fabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 6934251740 bytes\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"tmp/neg_issues.pkl\"\n",
    "if os.path.exists(file_path):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File size: {file_size} bytes\")\n",
    "    \n",
    "    if file_size == 0:\n",
    "        print(\"Error: File is empty.\")\n",
    "    else:\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                issues = pickle.load(file)\n",
    "            print(\"Data loaded successfully.\")\n",
    "        except (pickle.PickleError, EOFError) as e:\n",
    "            print(f\"Error loading the pickle file (corrupted or incomplete): {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "df = pd.DataFrame(sum(issues.values(), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c2b994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "issues.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c1c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100_000  # Adjust based on your memory\n",
    "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f01ab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/36\n",
      "Processing chunk 2/36\n",
      "Processing chunk 3/36\n",
      "Processing chunk 4/36\n",
      "Processing chunk 5/36\n",
      "Processing chunk 6/36\n",
      "Processing chunk 7/36\n",
      "Processing chunk 8/36\n",
      "Processing chunk 9/36\n",
      "Processing chunk 10/36\n",
      "Processing chunk 11/36\n",
      "Processing chunk 12/36\n",
      "Processing chunk 13/36\n",
      "Processing chunk 14/36\n",
      "Processing chunk 15/36\n",
      "Processing chunk 16/36\n",
      "Processing chunk 17/36\n",
      "Processing chunk 18/36\n",
      "Processing chunk 19/36\n",
      "Processing chunk 20/36\n",
      "Processing chunk 21/36\n",
      "Processing chunk 22/36\n",
      "Processing chunk 23/36\n",
      "Processing chunk 24/36\n",
      "Processing chunk 25/36\n",
      "Processing chunk 26/36\n",
      "Processing chunk 27/36\n",
      "Processing chunk 28/36\n",
      "Processing chunk 29/36\n",
      "Processing chunk 30/36\n",
      "Processing chunk 31/36\n",
      "Processing chunk 32/36\n",
      "Processing chunk 33/36\n",
      "Processing chunk 34/36\n",
      "Processing chunk 35/36\n",
      "Processing chunk 36/36\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "partial_datasets = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "    dataset_chunk = Dataset.from_pandas(chunk, preserve_index=False)\n",
    "    partial_datasets.append(dataset_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18cf5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = concatenate_datasets(partial_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730279b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b6bd096d8a4cb98c07aee4918826fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3fd3b7683c4fac95f8471064666b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa29b2857b864ac0a875246fe218f987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101012226723443ab45681b38794f9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c58a4dbb1764a64a70290b7815ff31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cb40fdb9b243c79bd4ed3b2991828f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ec4577f0584d549c0b6267e7a8639d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a28aaa05a54b9e86860e2aca3a0b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4a3bc9ca924221a15ba88022df9da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0175dc81b580405f8dbd6f091ee9df92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1f0036a6f4472fa9e8668cab15bd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb673b71e654a2c90a98e0ee6d7b0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317f2d9c0cf54acdafc3f788dedff056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739e2311715d4f5d9eebd4d6dd52d71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c9590d31014c459434a62623c108ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43009a711b334b55acc1d9dde3ca1f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/240 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Eathus/github-issues-negatives-max/commit/c61218277e21ca4b8167e47ba39b4dbb816ff349', commit_message='Upload dataset', commit_description='', oid='c61218277e21ca4b8167e47ba39b4dbb816ff349', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Eathus/github-issues-negatives-max', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Eathus/github-issues-negatives-max'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.push_to_hub(\"Eathus/github-issues-negatives-max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49173874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import gc\n",
    "\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88af6daf927b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "dataset.push_to_hub(\"Eathus/github-issues-negatives-max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d70e4",
   "metadata": {},
   "source": [
    "## Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ce4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "from ast import literal_eval  # Needed if issue_owner_repo is stored as string\n",
    "\n",
    "dataset = datasets.load_dataset(\"Eathus/github-issues-negatives\")\n",
    "\n",
    "# Assuming it's in the default \"train\" split\n",
    "df = dataset[\"train\"].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a75092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_owner_repo'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56607b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Load the dataset from Hugging Face Hub\n",
    "dataset = datasets.load_dataset(\"Eathus/github-issues-negatives\")\n",
    "\n",
    "# Assuming it's in the default \"train\" split\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "# 2. Convert back to the original issues dictionary structure\n",
    "issues = {}\n",
    "\n",
    "# If issue_owner_repo was stored as string (like \"(owner, repo)\"), convert it back to tuple\n",
    "# If it was properly stored as tuple, you can skip the literal_eval part\n",
    "df['issue_owner_repo'] = df['issue_owner_repo'].apply(lambda arr: (arr[0], arr[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3526d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['issue_owner_repo'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by owner/repo tuple and convert each group back to the original item format\n",
    "for (owner, repo), group in df.groupby('issue_owner_repo'):\n",
    "    repo_issues = []\n",
    "    for _, row in group.iterrows():\n",
    "        item = {\n",
    "            \"issue_owner_repo\": (owner, repo),\n",
    "            \"issue_body\": issue['body'],\n",
    "            \"issue_title\":issue['title'],\n",
    "            \"issue_comments_url\": issue['comments_url'],\n",
    "            \"issue_comments_count\": issue['comments'],\n",
    "            \"issue_created_at\": issue['created_at'],\n",
    "            \"issue_updated_at\": issue[\"updated_at\"],\n",
    "            \"issue_html_url\": issue[\"html_url\"],\n",
    "            \"issue_github_id\": issue[\"id\"],\n",
    "            \"issue_number\": issue[\"number\"],\n",
    "        }\n",
    "        repo_issues.append(item)\n",
    "    issues[(owner, repo)] = repo_issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(issues.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now you can save it back to pickle if needed\n",
    "file_path = \"tmp/neg_issues.pkl\"\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(issues, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file.flush()\n",
    "    os.fsync(file.fileno())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mthesis_cpyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
